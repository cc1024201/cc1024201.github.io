<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://cc1024201.github.io</id>
    <title>zhcao.blog</title>
    <updated>2021-06-14T13:57:49.676Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://cc1024201.github.io"/>
    <link rel="self" href="https://cc1024201.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://cc1024201.github.io/images/avatar.png</logo>
    <icon>https://cc1024201.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, zhcao.blog</rights>
    <entry>
        <title type="html"><![CDATA[网络编程]]></title>
        <id>https://cc1024201.github.io/post/wang-luo-bian-cheng/</id>
        <link href="https://cc1024201.github.io/post/wang-luo-bian-cheng/">
        </link>
        <updated>2021-06-13T03:47:22.000Z</updated>
        <content type="html"><![CDATA[<p>如果我问你一些关于网络编程方面的问题，你会怎样回答呢？</p>
<blockquote>
<p>大家经常说的四层、七层，分别指的是什么？<br>
TCP 三次握手是什么，TIME_WAIT 是怎么发生的？CLOSE_WAIT 又是什么状态？<br>
Linux 下的 epoll 解决的是什么问题？如何使用 epoll 写出高性能的网络程序？<br>
什么是网络事件驱动模型？Reactor 模式又是什么？</p>
</blockquote>
<p>很多情况下，我们都希望尽可能详尽地学习网络编程，面面俱到，但奈何头绪太多。<br>
比如流量控制和拥塞控制只是网络编程一小部分的内容，进程、线程、多路复用、异步 I/O 这些概念一摆出来，又会让人一头雾水。</p>
<p>很多人在理论部分折了戟，干脆跑向了另一个极端，转而去学习框架，快速上手。事实上，理论是基石，框架则是站在一个更为抽象的角度来看待网络编程问题。框架的产生或是为了实现跨平台支持，例如 JDK。没有理论为底，框架也只是空中楼阁。直接学习框架反而会更加摸不着头脑，对网络编程也很难有实打实的收获。</p>
<p>事实上，学习高性能网络编程，掌握两个核心要点就可以了：第一就是理解网络协议，并在这个基础上和操作系统内核配合，感知各种网络 I/O 事件；第二就是学会使用线程处理并发。</p>
<p>要学好网络编程，需要达到以下三个层次。<br>
第一个层次，充分理解 TCP/IP 网络模型和协议。（套接字，套接字缓冲区，拥塞控制，数据包和数据流，本地套接字（UNIX 域套接字）等）<br>
第二个层次，结合对协议的理解，增强对各种异常情况的优雅处理能力。（TCP 数据流的处理，半关闭的连接，TCP 连接有效性的侦测，处理各种异常情况等，这些问题决定了程序的健壮性。）<br>
第三个层次，写出可以支持大规模高并发的网络处理程序。（深入研究 C10K 问题，引入进程、线程、多路复用、非阻塞、异步、事件驱动等现代高性能网络编程所需要的技术。）</p>
<h1 id="tcpip-linux">TCP/IP &amp; Linux</h1>
<h2 id="osi-tcpip">OSI &amp; TCP/IP</h2>
<p><img src="https://cc1024201.github.io/post-images/1623587113767.jpg" alt="" loading="lazy"><br>
<img src="https://cc1024201.github.io/post-images/1623587120954.jpg" alt="" loading="lazy"></p>
<h2 id="linux">Linux</h2>
<h3 id="操作系统内核网络协议栈">操作系统内核网络协议栈</h3>
<h1 id="网络编程模型">网络编程模型</h1>
<h2 id="客户端-服务器网络编程模型">客户端-服务器网络编程模型</h2>
<figure data-type="image" tabindex="1"><img src="https://cc1024201.github.io/post-images/1623591951370.PNG" alt="" loading="lazy"></figure>
<ol>
<li>当一个客户端需要服务时，比如网络购物下单，它会向服务器端发送一个请求。注意，<br>
这个请求是按照双方约定的格式来发送的，以便保证服务器端是可以理解的；</li>
<li>服务器端收到这个请求后，会根据双方约定的格式解释它，并且以合适的方式进行操<br>
作，比如调用数据库操作来创建一个购物单；</li>
<li>服务器端完成处理请求之后，会给客户端发送一个响应，比如向客户端发送购物单的实<br>
际付款额，然后等待客户端的下一步操作；</li>
<li>客户端收到响应并进行处理，比如在手机终端上显示该购物单的实际付款额，并且让用<br>
户选择付款方式。</li>
</ol>
<p>服务器端需要在一开始就监听在一个众所周知的端口上，等待客户端发送请求，一旦有客户端连接建立，服器端就会消耗一定的计算机资源为它服务，服务器端是需要同时为成千上万的客户端服务的。<br>
客户端相对来说更为简单，它向服务器端的监听端口发起连接请求，连接建立之后，通过连接通路和服务器端进行通信。<br>
还有一点需要强调的是，无论是客户端，还是服务器端，它们运行的单位都是进程（process），而不是机器。</p>
<h2 id="ip-和端口">IP 和端口</h2>
<p>一个连接可以通过客户端 - 服务器端的 IP 和端口唯一确定，这叫做套接字对，按照下面的<br>
四元组表示：（clientaddr:clientport, serveraddr: serverport)<br>
客户端 - 服务器之间的连接：<br>
<img src="https://cc1024201.github.io/post-images/1623592818501.PNG" alt="" loading="lazy"></p>
<h3 id="保留网段">保留网段</h3>
<p>国际标准组织在 IPv4 地址空间里面，专门划出了一些网段，这些网段不会用做公网上的 IP，而是仅仅保留做内部使用，我们把这些地址称作保留网段。<br>
三个保留网段，其可以容纳的计算机主机个数分别是 16777216 个、1048576 个和65536 个。<br>
<img src="https://cc1024201.github.io/post-images/1623592968338.PNG" alt="" loading="lazy"></p>
<h3 id="子网掩码">子网掩码</h3>
<h2 id="全球域名系统">全球域名系统</h2>
<figure data-type="image" tabindex="2"><img src="https://cc1024201.github.io/post-images/1623593169441.PNG" alt="" loading="lazy"></figure>
<h2 id="数据报和字节流">数据报和字节流</h2>
<p>TCP，又被叫做字节流套接字（Stream Socket）<br>
UDP 也有一个类似的叫法, 数据报套接字（Datagram Socket）<br>
一般分别以“SOCK_STREAM”与“SOCK_DGRAM”分别来表示 TCP 和 UDP 套接字。</p>
<h1 id="套接字和地址">套接字和地址</h1>
<p>socket 这个英文单词的原意是“插口”“插槽”， 在网络编程中，它的寓意是可以通过插口接入的方式，快速完成网络连接和数据收发。<br>
<img src="https://cc1024201.github.io/post-images/1623641589422.PNG" alt="" loading="lazy"><br>
先从右侧的服务器端开始看，因为在客户端发起连接请求之前，服务器端必须初始化好。右侧的图显示的是服务器端初始化的过程，首先初始化 socket，之后服务器端需要执行 bind 函数，将自己的服务能力绑定一个众所周知的地址和端口上，紧接着，服务器端执行 listen 操作，将原先的 socket 转化为服务端的 socket，服务端最后阻塞在 accept 上等待客户端请求的到来。<br>
此时，服务器端已经准备就绪。客户端需要先初始化 socket，再执行 connect 向服务器端的地址和端口发起连接请求，这里的地址和端口必须是客户端预先知晓的。这个过程，就是著名的TCP 三次握手（Three-way Handshake）。<br>
一旦三次握手完成，客户端和服务器端建立连接，就进入了数据传输过程。</p>
<p>具体来说，客户端进程向操作系统内核发起 write 字节流写操作，内核协议栈将字节流通过网络设备传输到服务器端，服务器端从内核得到信息，将字节流从内核读入到进程中，并开始业务逻辑的处理，完成之后，服务器端再将得到的结果以同样的方式写给客户端。可以看到，一旦连接建立，数据的传输就不再是单向的，而是双向的，这也是 TCP 的一个显著特性。</p>
<p>当客户端完成和服务器端的交互后，比如执行一次 Telnet 操作，或者一次 HTTP 请求，需要和服务器端断开连接时，就会执行 close 函数，操作系统内核此时会通过原先的连接链路向服务器端发送一个 FIN 包，服务器收到之后执行被动关闭，这时候整个链路处于半关闭状态，此后，服务器端也会执行 close 函数，整个链路才会真正关闭。半关闭的状态下，发起 close 请求的一方在没有收到对方 FIN 包之前都认为连接是正常的；而在全关闭的状态下，双方都感知连接已经关闭。</p>
<p>socket 的概念，请注意，以上所有的操作，都是通过socket 来完成的。无论是客户端的 connect，还是服务端的 accept，或者 read/write 操作等，socket 是我们用来建立连接，传输数据的唯一途径。</p>
<h2 id="套接字地址格式">套接字地址格式</h2>
<h3 id="通用套接字地址格式">通用套接字地址格式</h3>
<h3 id="ipv4-套接字格式地址">IPv4 套接字格式地址</h3>
<h3 id="ipv6-套接字地址格式">IPv6 套接字地址格式</h3>
<h3 id="几种套接字地址格式比较">几种套接字地址格式比较</h3>
<figure data-type="image" tabindex="3"><img src="https://cc1024201.github.io/post-images/1623642195531.PNG" alt="" loading="lazy"></figure>
<h1 id="tcp三次握手">TCP三次握手</h1>
<h2 id="服务端准备连接的过程">服务端准备连接的过程</h2>
<h3 id="创建套接字">创建套接字</h3>
<p>要创建一个可用的套接字，需要使用下面的函数：</p>
<blockquote>
<p>int socket(int domain, int type, int protocol)</p>
</blockquote>
<p>domain 就是指 PF_INET、PF_INET6 以及 PF_LOCAL 等，表示什么样的套接字。<br>
type 可用的值是：<br>
SOCK_STREAM: 表示的是字节流，对应 TCP；<br>
SOCK_DGRAM： 表示的是数据报，对应 UDP；<br>
SOCK_RAW: 表示的是原始套接字。</p>
<p>参数 protocol 原本是用来指定通信协议的，但现在基本废弃。因为协议已经通过前面两个参数指定完成。protocol 目前一般写成 0 即可。</p>
<h3 id="bind">bind</h3>
<p>创建出来的套接字如果需要被别人使用，就需要调用 bind 函数把套接字和套接字地址绑定。<br>
调用 bind 函数的方式如下：</p>
<blockquote>
<p>bind(int fd, sockaddr * addr, socklen_t len)</p>
</blockquote>
<p>我们需要注意到 bind 函数后面的第二个参数是通用地址格式sockaddr * addr。这里有一个地方值得注意，那就是虽然接收的是通用地址格式，实际上传入的参数可能是 IPv4、IPv6 或者本地套接字格式。bind 函数会根据 len 字段判断传入的参数 addr 该怎么解析，len 字段表示的就是传入的地址长度，它是一个可变值。</p>
<p>设置 bind 的时候，对地址和端口可以有多种处理方式。</p>
<p>我们可以把地址设置成本机的 IP 地址，这相当告诉操作系统内核，仅仅对目标 IP 是本机IP 地址的 IP 包进行处理。但是这样写的程序在部署时有一个问题，我们编写应用程序时并不清楚自己的应用程序将会被部署到哪台机器上。这个时候，可以利用通配地址的能力帮助我们解决这个问题。通配地址相当于告诉操作系统内核：“Hi，我可不挑活，只要目标地址是咱们的都可以。”比如一台机器有两块网卡，IP 地址分别是 202.61.22.55 和192.168.1.11，那么向这两个 IP 请求的请求包都会被我们编写的应用程序处理。<br>
那么该如何设置通配地址呢？<br>
对于 IPv4 的地址来说，使用 INADDR_ANY 来完成通配地址的设置；对于 IPv6 的地址来说，使用 IN6ADDR_ANY 来完成通配地址的设置。</p>
<p>除了地址，还有端口。如果把端口设置成 0，就相当于把端口的选择权交给操作系统内核来处理，操作系统内核会根据一定的算法选择一个空闲的端口，完成套接字的绑定。这在服务器端不常使用。<br>
一般来说，服务器端的程序一定要绑定到一个众所周知的端口上。</p>
<h3 id="listen">listen</h3>
<p>bind 函数只是让我们的套接字和地址关联，让服务器真正处于可接听的状态，这个过程需要依赖 listen 函数。</p>
<p>初始化创建的套接字，可以认为是一个&quot;主动&quot;套接字，其目的是之后主动发起请求（通过调用 connect 函数）。通过 listen 函数，可以将原来的&quot;主动&quot;套接字转换为&quot;被动&quot;套接字，告诉操作系统内核：“我这个套接字是用来等待用户请求的。”当然，操作系统内核会为此做好接收用户请求的一切准备，比如完成连接队列。</p>
<p>listen 函数的原型是这样的：</p>
<blockquote>
<p>int listen (int socketfd, int backlog)</p>
</blockquote>
<p>我来稍微解释一下。第一个参数 socketfd 为套接字描述符，第二个参数 backlog，官方的解释为未完成连接队列的大小，这个参数的大小决定了可以接收的并发数目。这个参数越大，并发数目理论上也会越大。但是参数过大也会占用过多的系统资源，一些系统，比如 Linux 并不允许对这个参数进行改变。对于 backlog 整个参数的设置有一些最佳实践。</p>
<h3 id="accept">accept</h3>
<p>当客户端的连接请求到达时，服务器端应答成功，连接建立，这个时候操作系统内核需要把这个事件通知到应用程序，并让应用程序感知到这个连接。<br>
accept 这个函数的作用就是连接建立之后，操作系统内核和应用程序之间的桥梁。它的原型是：</p>
<blockquote>
<p>int accept(int listensockfd, struct sockaddr *cliaddr, socklen_t *addrlen)</p>
</blockquote>
<p>函数的第一个参数 listensockfd 是套接字，可以叫它为 listen 套接字，因为这就是前面通过 bind，listen 一系列操作而得到的套接字。函数的返回值有两个部分，第一个部分 cliadd 是通过指针方式获取的客户端的地址，addrlen 告诉我们地址的大小，这可以理解成当我们拿起电话机时，看到了来电显示，知道了对方的号码；另一个部分是函数的返回值，这个返回值是一个全新的描述字，代表了与客户端的连接。</p>
<p>这里一定要注意有两个套接字描述字，第一个是监听套接字描述字 listensockfd，它是作为输入参数存在的；第二个是返回的已连接套接字描述字。</p>
<p>你可能会问，为什么要把两个套接字分开呢？用一个不是挺好的么？</p>
<p>这里和打电话的情形非常不一样的地方就在于，打电话一旦有一个连接建立，别人是不能再打进来的，只会得到语音播报：“您拨的电话正在通话中。”而网络程序的一个重要特征就是并发处理，不可能一个应用程序运行之后只能服务一个客户。</p>
<p>所以监听套接字一直都存在，它是要为成千上万的客户来服务的，直到这个监听套接字关闭；而一旦一个客户和服务器连接成功，完成了 TCP 三次握手，操作系统内核就为这个客户生成一个已连接套接字，让应用服务器使用这个已连接套接字和客户进行通信处理。如果应用服务器完成了对这个客户的服务，比如一次网购下单，一次付款成功，那么关闭的就是已连接套接字，这样就完成了 TCP 连接的释放。请注意，这个时候释放的只是这一个客户连接，其它被服务的客户连接可能还存在。最重要的是，监听套接字一直都处于“监听”状态，等待新的客户请求到达并服务。</p>
<h2 id="客户端发起连接的过程">客户端发起连接的过程</h2>
<p>bind、listen 以及 accept 的过程，是典型的服务器端的过程。<br>
第一步还是和服务端一样，要建立一个套接字，方法和前面是一样的。<br>
不一样的是客户端需要调用 connect 向服务端发起请求。</p>
<h3 id="connect">connect</h3>
<p>客户端和服务器端的连接建立，是通过 connect 函数完成的。这是 connect 的构建函数：</p>
<blockquote>
<p>int connect(int sockfd, const struct sockaddr *servaddr, socklen_t addrlen)</p>
</blockquote>
<p>函数的第一个参数 sockfd 是连接套接字，通过前面讲述的 socket 函数创建。第二个、第三个参数 servaddr 和 addrlen 分别代表指向套接字地址结构的指针和该结构的大小。套接字地址结构必须含有服务器的 IP 地址和端口号。</p>
<p>客户在调用函数 connect 前不必非得调用 bind 函数，因为如果需要的话，内核会确定源 IP 地址，并按照一定的算法选择一个临时端口作为源端口。</p>
<p>如果是 TCP 套接字，那么调用 connect 函数将激发 TCP 的三次握手过程，而且仅在连接建立成功或出错时才返回。其中出错返回可能有以下几种情况：</p>
<ol>
<li>三次握手无法建立，客户端发出的 SYN 包没有任何响应，于是返回 TIMEOUT 错误。这种情况比较常见的原因是对应的服务端 IP 写错。</li>
<li>客户端收到了 RST（复位）回答，这时候客户端会立即返回 CONNECTION REFUSED 错误。这种情况比较常见于客户端发送连接请求时的请求端口写错，因为 RST 是 TCP 在发生错误时发送的一种 TCP 分节。产生 RST 的三个条件是：目的地为某端口的 SYN 到达，然而该端口上没有正在监听的服务器（如前所述）；TCP 想取消一个已有连接；TCP 接收到一个根本不存在的连接上的分节。</li>
<li>客户发出的 SYN 包在网络上引起了&quot;destination unreachable&quot;，即目的不可达的错误。这种情况比较常见的原因是客户端和服务器端路由不通。</li>
</ol>
<h2 id="著名的-tcp-三次握手">著名的 TCP 三次握手</h2>
<p><img src="https://cc1024201.github.io/post-images/1623644207272.PNG" alt="" loading="lazy"><br>
这里我们使用的网络编程模型都是阻塞式的。所谓阻塞式，就是调用发起后不会直接返回，由操作系统内核处理之后才会返回。</p>
<h3 id="tcp-三次握手的解读">TCP 三次握手的解读</h3>
<p>我们先看一下最初的过程，服务器端通过 socket，bind 和 listen 完成了被动套接字的准备工作，被动的意思就是等着别人来连接，然后调用 accept，就会阻塞在这里，等待客户端的连接来临；客户端通过调用 socket 和 connect 函数之后，也会阻塞。接下来的事情是由操作系统内核完成的，更具体一点的说，是操作系统内核网络协议栈在工作。</p>
<p>下面是具体的过程：</p>
<ol>
<li>客户端的协议栈向服务器端发送了 SYN 包，并告诉服务器端当前发送序列号 j，客户端进入 SYNC_SENT 状态；</li>
<li>服务器端的协议栈收到这个包之后，和客户端进行 ACK 应答，应答的值为 j+1，表示对 SYN 包 j 的确认，同时服务器也发送一个 SYN 包，告诉客户端当前我的发送序列号为 k，服务器端进入 SYNC_RCVD 状态；</li>
<li>客户端协议栈收到 ACK 之后，使得应用程序从 connect 调用返回，表示客户端到服务器端的单向连接建立成功，客户端的状态为 ESTABLISHED，同时客户端协议栈也会对服务器端的 SYN 包进行应答，应答数据为 k+1；</li>
<li>应答包到达服务器端后，服务器端协议栈使得 accept 阻塞调用返回，这个时候服务器端到客户端的单向连接也建立成功，服务器端也进入 ESTABLISHED 状态。</li>
</ol>
<p>形象一点的比喻是这样的，有 A 和 B 想进行通话：</p>
<ol>
<li>A 先对 B 说：“喂，你在么？我在的，我的口令是 j。”</li>
<li>B 收到之后大声回答：“我收到你的口令 j 并准备好了，你准备好了吗？我的口令是 k。”</li>
<li>A 收到之后也大声回答：“我收到你的口令 k 并准备好了，我们开始吧。”<br>
可以看到，这样的应答过程总共进行了三次，这就是 TCP 连接建立之所以被叫为“三次握手”的原因了。</li>
</ol>
<h2 id="总结">总结</h2>
<ul>
<li>服务器端通过创建 socket，bind，listen 完成初始化，通过 accept 完成连接的建立。</li>
<li>客户端通过场景 socket，connect 发起连接建立请求。</li>
</ul>
<h1 id="使用套接字进行读写">使用套接字进行读写</h1>
<p>连接建立的根本目的是为了数据的收发。</p>
<h2 id="发送数据">发送数据</h2>
<p>发送数据时常用的有三个函数，分别是 write、send 和 sendmsg。</p>
<blockquote>
<p>ssize_t write (int socketfd, const void *buffer, size_t size)<br>
ssize_t send (int socketfd, const void *buffer, size_t size, int flags)<br>
ssize_t sendmsg(int sockfd, const struct msghdr *msg, int flags)</p>
</blockquote>
<p>每个函数都是单独使用的，使用的场景略有不同：</p>
<p>第一个函数是常见的文件写函数，如果把 socketfd 换成文件描述符，就是普通的文件写入。</p>
<p>如果想指定选项，发送带外数据，就需要使用第二个带 flag 的函数。所谓带外数据，是一种基于 TCP 协议的紧急数据，用于客户端 - 服务器在特定场景下的紧急处理。</p>
<p>如果想指定多重缓冲区传输数据，就需要使用第三个函数，以结构体 msghdr 的方式发送数据。</p>
<p>对于普通文件描述符而言，一个文件描述符代表了打开的一个文件句柄，通过调用 write 函数，操作系统内核帮我们不断地往文件系统中写入字节流。注意，写入的字节流大小通常和输入参数 size 的值是相同的，否则表示出错。</p>
<p>对于套接字描述符而言，它代表了一个双向连接，在套接字描述符上调用 write 写入的字节数有可能比请求的数量少，这在普通文件描述符情况下是不正常的。</p>
<p>产生这个现象的原因在于操作系统内核为读取和发送数据做了很多我们表面上看不到的工作。接下来我拿 write 函数举例，重点阐述发送缓冲区的概念。</p>
<h3 id="发送缓冲区">发送缓冲区</h3>
<p>你一定要建立一个概念，当 TCP 三次握手成功，TCP 连接成功建立后，操作系统内核会为每一个连接创建配套的基础设施，比如发送缓冲区。</p>
<p>发送缓冲区的大小可以通过套接字选项来改变，当我们的应用程序调用 write 函数时，实际所做的事情是把数据从应用程序中拷贝到操作系统内核的发送缓冲区中，并不一定是把数据通过套接字写出去。</p>
<p>这里有几种情况：</p>
<p>第一种情况很简单，操作系统内核的发送缓冲区足够大，可以直接容纳这份数据，那么皆大欢喜，我们的程序从 write 调用中退出，返回写入的字节数就是应用程序的数据大小。</p>
<p>第二种情况是，操作系统内核的发送缓冲区是够大了，不过还有数据没有发送完，或者数据发送完了，但是操作系统内核的发送缓冲区不足以容纳应用程序数据，在这种情况下，你预料的结果是什么呢？报错？还是直接返回？</p>
<p>操作系统内核并不会返回，也不会报错，而是应用程序被阻塞，也就是说应用程序在 write 函数调用处停留，不直接返回。术语“挂起”也表达了相同的意思，不过“挂起”是从操作系统内核角度来说的。</p>
<p>那么什么时候才会返回呢？</p>
<p>实际上，每个操作系统内核的处理是不同的。大部分 UNIX 系统的做法是一直等到可以把应用程序数据完全放到操作系统内核的发送缓冲区中，再从系统调用中返回。怎么理解呢？</p>
<p>别忘了，我们的操作系统内核是很聪明的，当 TCP 连接建立之后，它就开始运作起来。你可以把发送缓冲区想象成一条包裹流水线，有个聪明且忙碌的工人不断地从流水线上取出包裹（数据），这个工人会按照 TCP/IP 的语义，将取出的包裹（数据）封装成 TCP 的 MSS 包，以及 IP 的 MTU 包，最后走数据链路层将数据发送出去。这样我们的发送缓冲区就又空了一部分，于是又可以继续从应用程序搬一部分数据到发送缓冲区里，这样一直进行下去，到某一个时刻，应用程序的数据可以完全放置到发送缓冲区里。在这个时候，write 阻塞调用返回。注意返回的时刻，应用程序数据并没有全部被发送出去，发送缓冲区里还有部分数据，这部分数据会在稍后由操作系统内核通过网络发送出去。<br>
<img src="https://cc1024201.github.io/post-images/1623652863886.PNG" alt="" loading="lazy"></p>
<h2 id="读取数据">读取数据</h2>
<p>我们可以注意到，套接字描述本身和本地文件描述符并无区别，在 UNIX 的世界里万物都是文件，这就意味着可以将套接字描述符传递给那些原先为处理本地文件而设计的函数。这些函数包括 read 和 write 交换数据的函数。</p>
<h3 id="read-函数">read 函数</h3>
<p>这个函数的原型如下：</p>
<blockquote>
<p>ssize_t read (int socketfd, void *buffer, size_t size)</p>
</blockquote>
<p>read 函数要求操作系统内核从套接字描述字 socketfd读取最多多少个字节（size），并将结果存储到 buffer 中。返回值告诉我们实际读取的字节数目，也有一些特殊情况，如果返回值为 0，表示 EOF（end-of-file），这在网络中表示对端发送了 FIN 包，要处理断连的情况；如果返回值为 -1，表示出错。当然，如果是非阻塞 I/O，情况会略有不同。</p>
<p>发送成功仅仅表示的是数据被拷贝到了发送缓冲区中，并不意味着连接对端已经收到所有的数据。至于什么时候发送到对端的接收缓冲区，或者更进一步说，什么时候被对方应用程序缓冲所接收，对我们而言完全都是透明的。</p>
<h2 id="总结-2">总结</h2>
<ul>
<li>对于 send 来说，返回成功仅仅表示数据写到发送缓冲区成功，并不表示对端已经成功收到。</li>
<li>对于 read 来说，需要循环读取数据，并且需要考虑 EOF 等异常条件。</li>
</ul>
<h1 id="udp">UDP</h1>
<h2 id="udp编程">UDP编程</h2>
<p><img src="https://cc1024201.github.io/post-images/1623653152029.PNG" alt="" loading="lazy"><br>
我们看到服务器端创建 UDP 套接字之后，绑定到本地端口，调用 recvfrom 函数等待客户端的报文发送；客户端创建套接字之后，调用 sendto 函数往目标地址和端口发送 UDP 报文，然后客户端和服务器端进入互相应答过程。</p>
<p>recvfrom 和 sendto 是 UDP 用来接收和发送报文的两个主要函数：</p>
<blockquote>
<p>ssize_t recvfrom(int sockfd, void *buff, size_t nbytes, int flags,<br>
　　　　　　　　　　struct sockaddr *from, socklen_t *addrlen);<br>
ssize_t sendto(int sockfd, const void *buff, size_t nbytes, int flags,<br>
const struct sockaddr *to, socklen_t *addrlen);</p>
</blockquote>
<p>我们先来看一下 recvfrom 函数。</p>
<p>sockfd、buff 和 nbytes 是前三个参数。sockfd 是本地创建的套接字描述符，buff 指向本地的缓存，nbytes 表示最大接收数据字节。</p>
<p>第四个参数 flags 是和 I/O 相关的参数，这里我们还用不到，设置为 0。</p>
<p>后面两个参数 from 和 addrlen，实际上是返回对端发送方的地址和端口等信息，这和 TCP 非常不一样，TCP 是通过 accept 函数拿到的描述字信息来决定对端的信息。另外 UDP 报文每次接收都会获取对端的信息，也就是说报文和报文之间是没有上下文的。</p>
<p>函数的返回值告诉我们实际接收的字节数。</p>
<p>接下来看一下 sendto 函数。</p>
<p>sendto 函数中的前三个参数为 sockfd、buff 和 nbytes。sockfd 是本地创建的套接字描述符，buff 指向发送的缓存，nbytes 表示发送字节数。第四个参数 flags 依旧设置为 0。</p>
<p>后面两个参数 to 和 addrlen，表示发送的对端地址和端口等信息。</p>
<p>函数的返回值告诉我们实际接收的字节数。</p>
<h2 id="总结-3">总结</h2>
<ul>
<li>UDP 是无连接的数据报程序，和 TCP 不同，不需要三次握手建立一条连接。</li>
<li>UDP 程序通过 recvfrom 和 sendto 函数直接收发数据报报文。</li>
</ul>
<h1 id="本地套接字">本地套接字</h1>
<p>本地套接字是 IPC，也就是本地进程间通信的一种实现方式。除了本地套接字以外，其它技术，诸如管道、共享消息队列等也是进程间通信的常用方法，但因为本地套接字开发便捷，接受度高，所以普遍适用于在同一台主机上进程间通信的各种场景。</p>
<h2 id="本地套接字概述">本地套接字概述</h2>
<p>本地套接字是一种特殊类型的套接字，和 TCP/UDP 套接字不同。TCP/UDP 即使在本地地址通信，也要走系统网络协议栈，而本地套接字，严格意义上说提供了一种单主机跨进程间调用的手段，减少了协议栈实现的复杂度，效率比 TCP/UDP 套接字都要高许多。类似的 IPC 机制还有 UNIX 管道、共享内存和 RPC 调用等。</p>
<h2 id="本地字节流套接字">本地字节流套接字</h2>
<h2 id="本地数据报套接字">本地数据报套接字</h2>
<h2 id="总结-4">总结</h2>
<ul>
<li>本地套接字的编程接口和 IPv4、IPv6 套接字编程接口是一致的，可以支持字节流和数据报两种协议。</li>
<li>本地套接字的实现效率大大高于 IPv4 和 IPv6 的字节流、数据报套接字实现。</li>
</ul>
<h1 id="网络工具">网络工具</h1>
<h2 id="ping">ping</h2>
<p>在网络上用来完成对网络连通性的探测<br>
基于一种叫做 ICMP 的协议开发的，ICMP 又是一种基于 IP 协议的控制协议，翻译为网际控制协议</p>
<h2 id="ifconfig">ifconfig</h2>
<p>显示当前系统中的所有网络设备，通俗一点的说，就是网卡列表。</p>
<h2 id="netstat-lsof">netstat &amp; lsof</h2>
<h2 id="tcpdump">tcpdump</h2>
<h2 id="总结-5">总结</h2>
<ul>
<li>ping 可以用来帮助我们进行网络连通性的探测。</li>
<li>ifconfig，用来显示当前系统中的所有网络设备。</li>
<li>netstat 和 lsof 可以查看活动的连接状况。</li>
<li>tcpdump 可以对各种奇怪的环境进行抓包，进而帮我们了解报文，排查问题。</li>
</ul>
<h1 id="time_wait">TIME_WAIT</h1>
<p>在四次挥手的过程中，发起连接断开的一方会有一段时间处于 TIME_WAIT 的状态</p>
<h2 id="time_wait-发生的场景">TIME_WAIT 发生的场景</h2>
<p>应用服务需要通过发起 TCP 连接对外提供服务。每个连接会占用一个本地端口，当在高并发的情况下，TIME_WAIT 状态的连接过多，多到把本机可用的端口耗尽，应用服务对外表现的症状，就是不能正常工作了。当过了一段时间之后，处于 TIME_WAIT 的连接被系统回收并关闭后，释放出本地端口可供使用，应用服务对外表现为，可以正常工作。这样周而复始，便会出现了一会儿不可以，过一两分钟又可以正常工作的现象。</p>
<p>那么为什么会产生这么多的 TIME_WAIT 连接呢？</p>
<p>这要从 TCP 的四次挥手说起。<br>
<img src="https://cc1024201.github.io/post-images/1623654452665.PNG" alt="" loading="lazy"></p>
<p>TCP 连接终止时，主机 1 先发送 FIN 报文，主机 2 进入 CLOSE_WAIT 状态，并发送一个 ACK 应答，同时，主机 2 通过 read 调用获得 EOF，并将此结果通知应用程序进行主动关闭操作，发送 FIN 报文。主机 1 在接收到 FIN 报文后发送 ACK 应答，此时主机 1 进入 TIME_WAIT 状态。</p>
<p>主机 1 在 TIME_WAIT 停留持续时间是固定的，是最长分节生命期 MSL（maximum segment lifetime）的两倍，一般称之为 2MSL。和大多数 BSD 派生的系统一样，Linux 系统里有一个硬编码的字段，名称为TCP_TIMEWAIT_LEN，其值为 60 秒。也就是说，Linux 系统停留在 TIME_WAIT 的时间为固定的 60 秒。</p>
<p>你一定要记住一点，只有发起连接终止的一方会进入 TIME_WAIT 状态。</p>
<h2 id="time_wait-的作用">TIME_WAIT 的作用</h2>
<p>首先，这样做是为了确保最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭。</p>
<p>TCP 在设计的时候，做了充分的容错性设计，比如，TCP 假设报文会出错，需要重传。在这里，如果图中主机 1 的 ACK 报文没有传输成功，那么主机 2 就会重新发送 FIN 报文。</p>
<p>如果主机 1 没有维护 TIME_WAIT 状态，而直接进入 CLOSED 状态，它就失去了当前状态的上下文，只能回复一个 RST 操作，从而导致被动关闭方出现错误。</p>
<p>现在主机 1 知道自己处于 TIME_WAIT 的状态，就可以在接收到 FIN 报文之后，重新发出一个 ACK 报文，使得主机 2 可以进入正常的 CLOSED 状态。</p>
<p>第二个理由和连接“化身”和报文迷走有关系，为了让旧连接的重复分节在网络中自然消失。</p>
<p>我们知道，在网络中，经常会发生报文经过一段时间才能到达目的地的情况，产生的原因是多种多样的，如路由器重启，链路突然出现故障等。如果迷走报文到达时，发现 TCP 连接四元组（源 IP，源端口，目的 IP，目的端口）所代表的连接不复存在，那么很简单，这个报文自然丢弃。</p>
<p>我们考虑这样一个场景，在原连接中断后，又重新创建了一个原连接的“化身”，说是化身其实是因为这个连接和原先的连接四元组完全相同，如果迷失报文经过一段时间也到达，那么这个报文会被误认为是连接“化身”的一个 TCP 分节，这样就会对 TCP 通信产生影响。<br>
<img src="https://cc1024201.github.io/post-images/1623654791080.PNG" alt="" loading="lazy"><br>
所以，TCP 就设计出了这么一个机制，经过 2MSL 这个时间，足以让两个方向上的分组都被丢弃，使得原来连接的分组在网络中都自然消失，再出现的分组一定都是新化身所产生的。</p>
<p>划重点，2MSL 的时间是从主机 1 接收到 FIN 后发送 ACK 开始计时的；如果在 TIME_WAIT 时间内，因为主机 1 的 ACK 没有传输到主机 2，主机 1 又接收到了主机 2 重发的 FIN 报文，那么 2MSL 时间将重新计时。道理很简单，因为 2MSL 的时间，目的是为了让旧连接的所有报文都能自然消亡，现在主机 1 重新发送了 ACK 报文，自然需要重新计时，以便防止这个 ACK 报文对新可能的连接化身造成干扰。</p>
<h2 id="time_wait-的危害">TIME_WAIT 的危害</h2>
<p>过多的 TIME_WAIT 的主要危害有两种。</p>
<p>第一是内存资源占用，这个目前看来不是太严重，基本可以忽略。</p>
<p>第二是对端口资源的占用，一个 TCP 连接至少消耗一个本地端口。要知道，端口资源也是有限的，一般可以开启的端口为 32768～61000 ，也可以通过net.ipv4.ip_local_port_range指定，如果 TIME_WAIT 状态过多，会导致无法创建新连接。</p>
<h2 id="如何优化-time_wait">如何优化 TIME_WAIT？</h2>
<p>net.ipv4.tcp_max_tw_buckets<br>
一个暴力的方法是通过 sysctl 命令，将系统值调小。这个值默认为 18000，当系统中处于 TIME_WAIT 的连接一旦超过这个值时，系统就会将所有的 TIME_WAIT 连接状态重置，并且只打印出警告信息。这个方法过于暴力，而且治标不治本，带来的问题远比解决的问题多，不推荐使用。</p>
<p>调低 TCP_TIMEWAIT_LEN，重新编译系统<br>
这个方法是一个不错的方法，缺点是需要“一点”内核方面的知识，能够重新编译内核。我想这个不是大多数人能接受的方式。</p>
<p>SO_LINGER 的设置</p>
<p>net.ipv4.tcp_tw_reuse：更安全的设置<br>
TCP 协议也在与时俱进，RFC 1323 中实现了 TCP 拓展规范，以便保证 TCP 的高可用，并引入了新的 TCP 选项，两个 4 字节的时间戳字段，用于记录 TCP 发送方的当前时间戳和从对端接收到的最新时间戳。由于引入了时间戳，我们在前面提到的 2MSL 问题就不复存在了，因为重复的数据包会因为时间戳过期被自然丢弃。</p>
<h2 id="总结-6">总结</h2>
<ul>
<li>TIME_WAIT 的引入是为了让 TCP 报文得以自然消失，同时为了让被动关闭方能够正常关闭；</li>
<li>不要试图使用SO_LINGER设置套接字选项，跳过 TIME_WAIT；</li>
<li>现代 Linux 系统引入了更安全可控的方案，可以帮助我们尽可能地复用 TIME_WAIT 状态的连接。</li>
</ul>
<h1 id="优雅地关闭还是粗暴地关闭">优雅地关闭还是粗暴地关闭 ?</h1>
<p>一个 TCP 连接需要经过三次握手进入数据传输阶段，最后来到连接关闭阶段。在最后的连接关闭阶段，我们需要重点关注的是“半连接”状态。</p>
<p>因为 TCP 是双向的，这里说的方向，指的是数据流的写入 - 读出的方向。</p>
<p>比如客户端到服务器端的方向，指的是客户端通过套接字接口，向服务器端发送 TCP 报文；而服务器端到客户端方向则是另一个传输方向。在绝大数情况下，TCP 连接都是先关闭一个方向，此时另外一个方向还是可以正常进行数据传输。</p>
<p>举个例子，客户端主动发起连接的中断，将自己到服务器端的数据流方向关闭，此时，客户端不再往服务器端写入数据，服务器端读完客户端数据后就不会再有新的报文到达。但这并不意味着，TCP 连接已经完全关闭，很有可能的是，服务器端正在对客户端的最后报文进行处理，比如去访问数据库，存入一些数据；或者是计算出某个客户端需要的值，当完成这些操作之后，服务器端把结果通过套接字写给客户端，我们说这个套接字的状态此时是“半关闭”的。最后，服务器端才有条不紊地关闭剩下的半个连接，结束这一段 TCP 连接的使命。</p>
<h2 id="close-函数">close 函数</h2>
<blockquote>
<p>int close(int sockfd)</p>
</blockquote>
<p>这个函数会对套接字引用计数减一，一旦发现套接字引用计数到 0，就会对套接字进行彻底释放，并且会关闭TCP 两个方向的数据流。</p>
<p>因为套接字可以被多个进程共享，你可以理解为我们给每个套接字都设置了一个积分，如果我们通过 fork 的方式产生子进程，套接字就会积分 +1， 如果我们调用一次 close 函数，套接字积分就会 -1。这就是套接字引用计数的含义。</p>
<p>close 函数具体是如何关闭两个方向的数据流呢？</p>
<p>在输入方向，系统内核会将该套接字设置为不可读，任何读操作都会返回异常。</p>
<p>在输出方向，系统内核尝试将发送缓冲区的数据发送给对端，并最后向对端发送一个 FIN 报文，接下来如果再对该套接字进行写操作会返回异常。</p>
<p>如果对端没有检测到套接字已关闭，还继续发送报文，就会收到一个 RST 报文，告诉对端：“Hi, 我已经关闭了，别再给我发数据了。”</p>
<p>我们会发现，close 函数并不能帮助我们关闭连接的一个方向，那么如何在需要的时候关闭一个方向呢？幸运的是，设计 TCP 协议的人帮我们想好了解决方案，这就是 shutdown 函数。</p>
<h2 id="shutdown-函数">shutdown 函数</h2>
<blockquote>
<p>int shutdown(int sockfd, int howto)</p>
</blockquote>
<p>howto 是这个函数的设置选项，它的设置有三个主要选项：</p>
<ul>
<li>SHUT_RD(0)：关闭连接的“读”这个方向，对该套接字进行读操作直接返回 EOF。从数据角度来看，套接字上接收缓冲区已有的数据将被丢弃，如果再有新的数据流到达，会对数据进行 ACK，然后悄悄地丢弃。也就是说，对端还是会接收到 ACK，在这种情况下根本不知道数据已经被丢弃了。</li>
<li>SHUT_WR(1)：关闭连接的“写”这个方向，这就是常被称为”半关闭“的连接。此时，不管套接字引用计数的值是多少，都会直接关闭连接的写方向。套接字上发送缓冲区已有的数据将被立即发送出去，并发送一个 FIN 报文给对端。应用程序如果对该套接字进行写操作会报错。</li>
<li>SHUT_RDWR(2)：相当于 SHUT_RD 和 SHUT_WR 操作各一次，关闭套接字的读和写两个方向。</li>
</ul>
<h2 id="close-和-shutdown-的差别">close 和 shutdown 的差别</h2>
<p>第一个差别：close 会关闭连接，并释放所有连接对应的资源，而 shutdown 并不会释放掉套接字和所有的资源。</p>
<p>第二个差别：close 存在引用计数的概念，并不一定导致该套接字不可用；shutdown 则不管引用计数，直接使得该套接字不可用，如果有别的进程企图使用该套接字，将会受到影响。</p>
<p>第三个差别：close 的引用计数导致不一定会发出 FIN 结束报文，而 shutdown 则总是会发出 FIN 结束报文，这在我们打算关闭连接通知对端的时候，是非常重要的。</p>
<h1 id="检测连接状态">检测连接状态</h1>
<h2 id="tcp-keep-alive-选项">TCP Keep-Alive 选项</h2>
<p>定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个探测报文，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。</p>
<p>上述的可定义变量，分别被称为保活时间、保活时间间隔和保活探测次数。在 Linux 系统中，这些变量分别对应 sysctl 变量net.ipv4.tcp_keepalive_time、net.ipv4.tcp_keepalive_intvl、 net.ipv4.tcp_keepalve_probes，默认设置是 7200 秒（2 小时）、75 秒和 9 次探测。</p>
<p>如果开启了 TCP 保活，需要考虑以下几种情况：</p>
<p>第一种，对端程序是正常工作的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 TCP 保活时间会被重置，等待下一个 TCP 保活时间的到来。</p>
<p>第二种，对端程序崩溃并重启。当 TCP 保活的探测报文发送给对端后，对端是可以响应的，但由于没有该连接的有效信息，会产生一个 RST 报文，这样很快就会发现 TCP 连接已经被重置。</p>
<p>第三种，是对端程序崩溃，或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，TCP 会报告该 TCP 连接已经死亡。</p>
<p>TCP 保活机制默认是关闭的，当我们选择打开时，可以分别在连接的两个方向上开启，也可以单独在一个方向上开启。如果开启服务器端到客户端的检测，就可以在客户端非正常断连的情况下清除在服务器端保留的“脏数据”；而开启客户端到服务器端的检测，就可以在服务器无响应的情况下，重新发起连接。</p>
<h2 id="应用层探活">应用层探活</h2>
<p>如果使用 TCP 自身的 keep-Alive 机制，在 Linux 系统中，最少需要经过 2 小时 11 分 15 秒才可以发现一个“死亡”连接。这个时间是怎么计算出来的呢？其实是通过 2 小时，加上 75 秒乘以 9 的总和。实际上，对很多对时延要求敏感的系统中，这个时间间隔是不可接受的。</p>
<p>所以，必须在应用程序这一层来寻找更好的解决方案。</p>
<p>我们可以通过在应用程序中模拟 TCP Keep-Alive 机制，来完成在应用层的连接探活。</p>
<p>我们可以设计一个 PING-PONG 的机制，需要保活的一方，比如客户端，在保活时间达到后，发起对连接的 PING 操作，如果服务器端对 PING 操作有回应，则重新设置保活时间，否则对探测次数进行计数，如果最终探测次数达到了保活探测次数预先设置的值之后，则认为连接已经无效。</p>
<p>这里有两个比较关键的点：</p>
<p>第一个是需要使用定时器，这可以通过使用 I/O 复用自身的机制来实现；第二个是需要设计一个 PING-PONG 的协议。</p>
<h1 id="理解tcp协议中的动态数据传输">理解TCP协议中的动态数据传输</h1>
<h2 id="流量控制和生产者-消费者模型">流量控制和生产者 - 消费者模型</h2>
<p>我们可以把理想中的 TCP 协议可以想象成一队运输货物的货车，运送的货物就是 TCP 数据包，这些货车将数据包从发送端运送到接收端，就这样不断周而复始。</p>
<p>我们仔细想一下，货物达到接收端之后，是需要卸货处理、登记入库的，接收端限于自己的处理能力和仓库规模，是不可能让这队货车以不可控的速度发货的。接收端肯定会和发送端不断地进行信息同步，比如接收端通知发送端：“后面那 20 车你给我等等，等我这里腾出地方你再继续发货。”</p>
<p>其实这就是发送窗口和接收窗口的本质，我管这个叫做“TCP 的生产者 - 消费者”模型。</p>
<p>发送窗口和接收窗口是 TCP 连接的双方，一个作为生产者，一个作为消费者，为了达到一致协同的生产 - 消费速率、而产生的算法模型实现。</p>
<p>发送窗口和接收窗口是 TCP 连接的双方，一个作为生产者，一个作为消费者，为了达到一致协同的生产 - 消费速率、而产生的算法模型实现。</p>
<p>说白了，作为 TCP 发送端，也就是生产者，不能忽略 TCP 的接收端，也就是消费者的实际状况，不管不顾地把数据包都传送过来。如果都传送过来，消费者来不及消费，必然会丢弃；而丢弃反过使得生产者又重传，发送更多的数据包，最后导致网络崩溃。</p>
<p>我想，理解了“TCP 的生产者 - 消费者”模型，再反过来看发送窗口和接收窗口的设计目的和方式，我们就会恍然大悟了。</p>
<h2 id="拥塞控制和数据传输">拥塞控制和数据传输</h2>
<p>TCP 的生产者 - 消费者模型，只是在考虑单个连接的数据传递，但是， TCP 数据包是需要经过网卡、交换机、核心路由器等一系列的网络设备的，网络设备本身的能力也是有限的，当多个连接的数据包同时在网络上传送时，势必会发生带宽争抢、数据丢失等，这样，TCP 就必须考虑多个连接共享在有限的带宽上，兼顾效率和公平性的控制，这就是拥塞控制的本质。</p>
<p>在 TCP 协议中，拥塞控制是通过拥塞窗口来完成的，拥塞窗口的大小会随着网络状况实时调整。</p>
<p>拥塞控制常用的算法有“慢启动”，它通过一定的规则，慢慢地将网络发送数据的速率增加到一个阈值。超过这个阈值之后，慢启动就结束了，另一个叫做“拥塞避免”的算法登场。在这个阶段，TCP 会不断地探测网络状况，并随之不断调整拥塞窗口的大小。</p>
<p>现在你可以发现，在任何一个时刻，TCP 发送缓冲区的数据是否能真正发送出去，至少取决于两个因素，一个是当前的发送窗口大小，另一个是拥塞窗口大小，而 TCP 协议中总是取两者中最小值作为判断依据。比如当前发送的字节为 100，发送窗口的大小是 200，拥塞窗口的大小是 80，那么取 200 和 80 中的最小值，就是 80，当前发送的字节数显然是大于拥塞窗口的，结论就是不能发送出去。</p>
<p>这里千万要分清楚发送窗口和拥塞窗口的区别。</p>
<p>发送窗口反应了作为单 TCP 连接、点对点之间的流量控制模型，它是需要和接收端一起共同协调来调整大小的；而拥塞窗口则是反应了作为多个 TCP 连接共享带宽的拥塞控制模型，它是发送端独立地根据网络状况来动态调整的。</p>
<h2 id="一些有趣的场景">一些有趣的场景</h2>
<p>我们考虑以下几个有趣的场景：</p>
<p>第一个场景，接收端处理得急不可待，比如刚刚读入了 100 个字节，就告诉发送端：“喂，我已经读走 100 个字节了，你继续发”，在这种情况下，你觉得发送端应该怎么做呢？</p>
<p>第二个场景是所谓的“交互式”场景，比如我们使用 telnet 登录到一台服务器上，或者使用 SSH 和远程的服务器交互，这种情况下，我们在屏幕上敲打了一个命令，等待服务器返回结果，这个过程需要不断和服务器端进行数据传输。这里最大的问题是，每次传输的数据可能都非常小，比如敲打的命令“pwd”，仅仅三个字符。这意味着什么？这就好比，每次叫了一辆大货车，只送了一个小水壶。在这种情况下，你又觉得发送端该怎么做才合理呢？</p>
<p>第三个场景是从接收端来说的。我们知道，接收端需要对每个接收到的 TCP 分组进行确认，也就是发送 ACK 报文，但是 ACK 报文本身是不带数据的分段，如果一直这样发送大量的 ACK 报文，就会消耗大量的带宽。之所以会这样，是因为 TCP 报文、IP 报文固有的消息头是不可或缺的，比如两端的地址、端口号、时间戳、序列号等信息， 在这种情形下，你觉得合理的做法是什么？</p>
<p>TCP 之所以复杂，就是因为 TCP 需要考虑的因素较多。像以上这几个场景，都是 TCP 需要考虑的情况，一句话概况就是如何有效地利用网络带宽。</p>
<p>第一个场景也被叫做糊涂窗口综合症，这个场景需要在接收端进行优化。也就是说，接收端不能在接收缓冲区空出一个很小的部分之后，就急吼吼地向发送端发送窗口更新通知，而是需要在自己的缓冲区大到一个合理的值之后，再向发送端发送窗口更新通知。这个合理的值，由对应的 RFC 规范定义。</p>
<p>第二个场景需要在发送端进行优化。这个优化的算法叫做 Nagle 算法，Nagle 算法的本质其实就是限制大批量的小数据包同时发送，为此，它提出，在任何一个时刻，未被确认的小数据包不能超过一个。这里的小数据包，指的是长度小于最大报文段长度 MSS 的 TCP 分组。这样，发送端就可以把接下来连续的几个小数据包存储起来，等待接收到前一个小数据包的 ACK 分组之后，再将数据一次性发送出去。</p>
<p>第三个场景，也是需要在接收端进行优化，这个优化的算法叫做延时 ACK。延时 ACK 在收到数据后并不马上回复，而是累计需要发送的 ACK 报文，等到有数据需要发送给对端时，将累计的 ACK捎带一并发送出去。当然，延时 ACK 机制，不能无限地延时下去，否则发送端误认为数据包没有发送成功，引起重传，反而会占用额外的网络带宽。</p>
<h2 id="禁用-nagle-算法">禁用 Nagle 算法</h2>
<p>有没有发现一个很奇怪的组合，即 Nagle 算法和延时 ACK 的组合。<br>
比如，客户端分两次将一个请求发送出去，由于请求的第一部分的报文未被确认，Nagle 算法开始起作用；同时延时 ACK 在服务器端起作用，假设延时时间为 200ms，服务器等待 200ms 后，对请求的第一部分进行确认；接下来客户端收到了确认后，Nagle 算法解除请求第二部分的阻止，让第二部分得以发送出去，服务器端在收到之后，进行处理应答，同时将第二部分的确认捎带发送出去。<br>
<img src="https://cc1024201.github.io/post-images/1623656592942.PNG" alt="" loading="lazy"><br>
从这张图中可以看到，Nagle 算法和延时确认组合在一起，增大了处理时延，实际上，两个优化彼此在阻止对方。<br>
在有些情况下 Nagle 算法并不适用， 比如对时延敏感的应用。<br>
幸运的是，我们可以通过对套接字的修改来关闭 Nagle 算法。<br>
值得注意的是，除非我们对此有十足的把握，否则不要轻易改变默认的 TCP Nagle 算法。因为在现代操作系统中，针对 Nagle 算法和延时 ACK 的优化已经非常成熟了，有可能在禁用 Nagle 算法之后，性能问题反而更加严重。</p>
<h2 id="将写操作合并">将写操作合并</h2>
<h1 id="udp也可以是已连接">UDP也可以是“已连接”</h1>
<h2 id="udp-connect-的作用">UDP connect 的作用</h2>
<h2 id="收发函数">收发函数</h2>
<h2 id="性能考虑">性能考虑</h2>
<h1 id="地址已经被使用">“地址已经被使用”</h1>
<h2 id="重用套接字选项">重用套接字选项</h2>
<p>现代 Linux 操作系统对此进行了一些优化。</p>
<p>第一种优化是新连接 SYN 告知的初始序列号，一定比 TIME_WAIT 老连接的末序列号大，这样通过序列号就可以区别出新老连接。</p>
<p>第二种优化是开启了 tcp_timestamps，使得新连接的时间戳比老连接的时间戳大，这样通过时间戳也可以区别出新老连接。</p>
<p>在这样的优化之下，一个 TIME_WAIT 的 TCP 连接可以忽略掉旧连接，重新被新的连接所使用。</p>
<p>这就是重用套接字选项，通过给套接字配置可重用属性，告诉操作系统内核，这样的 TCP 连接完全可以复用 TIME_WAIT 状态的连接。</p>
<blockquote>
<p>setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &amp;on, sizeof(on));</p>
</blockquote>
<p>SO_REUSEADDR 套接字选项，允许启动绑定在一个端口，即使之前存在一个和该端口一样的连接。前面的例子已经表明，在默认情况下，服务器端历经创建 socket、bind 和 listen 重启时，如果试图绑定到一个现有连接上的端口，bind 操作会失败，但是如果我们在创建 socket 和 bind 之间，使用上面的代码片段设置 SO_REUSEADDR 套接字选项，情况就会不同。</p>
<p>SO_REUSEADDR 套接字选项还有一个作用，那就是本机服务器如果有多个地址，可以在不同地址上使用相同的端口提供服务。</p>
<h2 id="最佳实践">最佳实践</h2>
<p>这里的最佳实践可以总结成一句话： 服务器端程序，都应该设置 SO_REUSEADDR 套接字选项，以便服务端程序可以在极短时间内复用同一个端口启动。</p>
<h1 id="如何理解tcp的流">如何理解TCP的“流”？</h1>
<h2 id="tcp-是一种流式协议">TCP 是一种流式协议</h2>
<p>在前面的章节中，我们讲的都是单个客户端 - 服务器的例子，可能会给你造成一种错觉，好像 TCP 是一种应答形式的数据传输过程，比如发送端一次发送 network 和 program 这样的报文，在前面的例子中，我们看到的结果基本是这样的：</p>
<p>发送端：network ----&gt; 接收端回应：Hi, network</p>
<p>发送端：program -----&gt; 接收端回应：Hi, program</p>
<p>这其实是一个假象，之所以会这样，是因为网络条件比较好，而且发送的数据也比较少。</p>
<p>为了让大家理解 TCP 数据是流式的这个特性，我们分别从发送端和接收端来阐述。</p>
<p>我们知道，在发送端，当我们调用 send 函数完成数据“发送”以后，数据并没有被真正从网络上发送出去，只是从应用程序拷贝到了操作系统内核协议栈中，至于什么时候真正被发送，取决于发送窗口、拥塞窗口以及当前发送缓冲区的大小等条件。也就是说，我们不能假设每次 send 调用发送的数据，都会作为一个整体完整地被发送出去。</p>
<p>如果我们考虑实际网络传输过程中的各种影响，假设发送端陆续调用 send 函数先后发送 network 和 program 报文，那么实际的发送很有可能是这个样子的。</p>
<p>第一种情况，一次性将 network 和 program 在一个 TCP 分组中发送出去，像这样：</p>
<p>...xxxnetworkprogramxxx...<br>
第二种情况，program 的部分随 network 在一个 TCP 分组中发送出去，像这样：</p>
<p>TCP 分组 1：</p>
<p>...xxxxxnetworkpro<br>
TCP 分组 2：</p>
<p>gramxxxxxxxxxx...<br>
第三种情况，network 的一部分随 TCP 分组被发送出去，另一部分和 program 一起随另一个 TCP 分组发送出去，像这样。</p>
<p>TCP 分组 1：</p>
<p>...xxxxxxxxxxxnet<br>
TCP 分组 2：</p>
<p>workprogramxxx...<br>
实际上类似的组合可以枚举出无数种。不管是哪一种，核心的问题就是，我们不知道 network 和 program 这两个报文是如何进行 TCP 分组传输的。换言之，我们在发送数据的时候，不应该假设“数据流和 TCP 分组是一种映射关系”。就好像在前面，我们似乎觉得 network 这个报文一定对应一个 TCP 分组，这是完全不正确的。</p>
<p>如果我们再来看客户端，数据流的特征更明显。</p>
<p>我们知道，接收端缓冲区保留了没有被取走的数据，随着应用程序不断从接收端缓冲区读出数据，接收端缓冲区就可以容纳更多新的数据。如果我们使用 recv 从接收端缓冲区读取数据，发送端缓冲区的数据是以字节流的方式存在的，无论发送端如何构造 TCP 分组，接收端最终受到的字节流总是像下面这样：</p>
<p>xxxxxxxxxxxxxxxxxnetworkprogramxxxxxxxxxxxx<br>
关于接收端字节流，有两点需要注意：</p>
<p>第一，这里 netwrok 和 program 的顺序肯定是会保持的，也就是说，先调用 send 函数发送的字节，总在后调用 send 函数发送字节的前面，这个是由 TCP 严格保证的；</p>
<p>第二，如果发送过程中有 TCP 分组丢失，但是其后续分组陆续到达，那么 TCP 协议栈会缓存后续分组，直到前面丢失的分组到达，最终，形成可以被应用程序读取的数据流。</p>
<h2 id="网络字节排序">网络字节排序</h2>
<p>高字节存放在起始地址，这个叫做大端字节序<br>
低字节存放在起始地址，这个叫做小端字节序</p>
<h2 id="报文读取和解析">报文读取和解析</h2>
<p>报文格式最重要的是如何确定报文的边界。常见的报文格式有两种方法，一种是发送端把要发送的报文长度预先通过报文告知给接收端；另一种是通过一些特殊的字符来进行边界的划分。</p>
<h3 id="显式编码报文长度">显式编码报文长度</h3>
<h3 id="特殊字符作为边界">特殊字符作为边界</h3>
<p>HTTP 是一个非常好的例子。<br>
<img src="https://cc1024201.github.io/post-images/1623659422601.PNG" alt="" loading="lazy"><br>
HTTP 通过设置回车符、换行符做为 HTTP 报文协议的边界。</p>
<h2 id="总结-7">总结</h2>
<p>TCP 数据流特性决定了字节流本身是没有边界的，一般我们通过显式编码报文长度的方式，以及选取特殊字符区分报文边界的方式来进行报文格式的设计。而对报文解析的工作就是要在知道报文格式的情况下，有效地对报文信息进行还原。</p>
<h1 id="tcp并不总是可靠的">TCP并不总是“可靠”的</h1>
<h2 id="故障模式总结">故障模式总结</h2>
<p>在实际情景中，我们会碰到各种异常的情况。在这里我把这几种异常情况归结为两大类：<br>
<img src="https://cc1024201.github.io/post-images/1623659586504.png" alt="" loading="lazy"></p>
<h3 id="网络中断造成的对端无-fin-包">网络中断造成的对端无 FIN 包</h3>
<p>很多原因都会造成网络中断，在这种情况下，TCP 程序并不能及时感知到异常信息。除非网络中的其他设备，如路由器发出一条 ICMP 报文，说明目的网络或主机不可达，这个时候通过 read 或 write 调用就会返回 Unreachable 的错误。</p>
<p>可惜大多数时候并不是如此，在没有 ICMP 报文的情况下，TCP 程序并不能理解感应到连接异常。如果程序是阻塞在 read 调用上，那么很不幸，程序无法从异常中恢复。这显然是非常不合理的，不过，我们可以通过给 read 操作设置超时来解决</p>
<p>如果程序先调用了 write 操作发送了一段数据流，接下来阻塞在 read 调用上，结果会非常不同。Linux 系统的 TCP 协议栈会不断尝试将发送缓冲区的数据发送出去，大概在重传 12 次、合计时间约为 9 分钟之后，协议栈会标识该连接异常，这时，阻塞的 read 调用会返回一条 TIMEOUT 的错误信息。如果此时程序还执着地往这条连接写数据，写操作会立即失败，返回一个 SIGPIPE 信号给应用程序。</p>
<h3 id="系统崩溃造成的对端无-fin-包">系统崩溃造成的对端无 FIN 包</h3>
<p>当系统突然崩溃，如断电时，网络连接上来不及发出任何东西。这里和通过系统调用杀死应用程序非常不同的是，没有任何 FIN 包被发送出来。</p>
<p>这种情况和网络中断造成的结果非常类似，在没有 ICMP 报文的情况下，TCP 程序只能通过 read 和 write 调用得到网络连接异常的信息，超时错误是一个常见的结果。</p>
<p>不过还有一种情况需要考虑，那就是系统在崩溃之后又重启，当重传的 TCP 分组到达重启后的系统，由于系统中没有该 TCP 分组对应的连接数据，系统会返回一个 RST 重置分节，TCP 程序通过 read 或 write 调用可以分别对 RST 进行错误处理。</p>
<p>如果是阻塞的 read 调用，会立即返回一个错误，错误信息为连接重置（Connection Resest）。</p>
<p>如果是一次 write 操作，也会立即失败，应用程序会被返回一个 SIGPIPE 信号。</p>
<h3 id="对端有-fin-包发出">对端有 FIN 包发出</h3>
<p>对端如果有 FIN 包发出，可能的场景是对端调用了 close 或 shutdown 显式地关闭了连接，也可能是对端应用程序崩溃，操作系统内核代为清理所发出的。从应用程序角度上看，无法区分是哪种情形。</p>
<p>阻塞的 read 操作在完成正常接收的数据读取之后，FIN 包会通过返回一个 EOF 来完成通知，此时，read 调用返回值为 0。这里强调一点，收到 FIN 包之后 read 操作不会立即返回。你可以这样理解，收到 FIN 包相当于往接收缓冲区里放置了一个 EOF 符号，之前已经在接收缓冲区的有效数据不会受到影响。</p>
<h4 id="read-直接感知-fin-包">read 直接感知 FIN 包</h4>
<h4 id="通过-write-产生-rstread-调用感知-rst">通过 write 产生 RST，read 调用感知 RST</h4>
<h4 id="向一个已关闭连接连续写最终导致-sigpipe">向一个已关闭连接连续写，最终导致 SIGPIPE</h4>
<h2 id="总结-8">总结</h2>
<p>我们意识到 TCP 并不是那么“可靠”的。我把故障分为两大类，一类是对端无 FIN 包，需要通过巡检或超时来发现；另一类是对端有 FIN 包发出，需要通过增强 read 或 write 操作的异常处理，帮助我们发现此类异常。</p>
<h1 id="检查数据的有效性">检查数据的有效性</h1>
<h2 id="对端的异常状况">对端的异常状况</h2>
<p>比如，通过 read 等调用时，可以通过对 EOF 的判断，随时防范对方程序崩溃。</p>
<p>比如，服务器完全崩溃，或者网络中断的情况下，此时，如果是阻塞套接字，会一直阻塞在 read 等调用上，没有办法感知套接字的异常。</p>
<p>其实有几种办法来解决这个问题。</p>
<p>第一个办法是给套接字的 read 操作设置超时，如果超过了一段时间就认为连接已经不存在。</p>
<p>第二个办法是添加对连接是否正常的检测。如果连接不正常，需要从当前 read 阻塞中返回并处理。</p>
<p>还有一个办法，那就是利用多路复用技术自带的超时能力，来完成对套接字 I/O 的检查，如果超过了预设的时间，就进入异常处理。</p>
<h2 id="缓冲区处理">缓冲区处理</h2>
<p>一个设计良好的网络程序，应该可以在随机输入的情况下表现稳定。不仅是这样，随着互联网的发展，网络安全也愈发重要，我们编写的网络程序能不能在黑客的刻意攻击之下表现稳定，也是一个重要考量因素。</p>
<p>很多黑客程序，会针对性地构建出一定格式的网络协议包，导致网络程序产生诸如缓冲区溢出、指针异常的后果，影响程序的服务能力，严重的甚至可以夺取服务器端的控制权，随心所欲地进行破坏活动，比如著名的 SQL 注入，就是通过针对性地构造出 SQL 语句，完成对数据库敏感信息的窃取。</p>
<p>所以，在网络程序的编写过程中，我们需要时时刻刻提醒自己面对的是各种复杂异常的场景，甚至是别有用心的攻击者，保持“防人之心不可无”的警惕。</p>
<p>那么程序都有可能出现哪几种漏洞呢？</p>
<h3 id="缓冲区溢出">缓冲区溢出</h3>
<p>所谓缓冲区溢出，是指计算机程序中出现的一种内存违规操作。本质是计算机程序向缓冲区填充的数据，超出了原本缓冲区设置的大小限制，导致了数据覆盖了内存栈空间的其他合法数据。这种覆盖破坏了原来程序的完整性</p>
<h1 id="如何理解tcp四次挥手">如何理解TCP四次挥手？</h1>
<p><img src="https://cc1024201.github.io/post-images/1623660234324.png" alt="" loading="lazy"><br>
首先，一方应用程序调用 close，我们称该方为主动关闭方，该端的 TCP 发送一个 FIN 包，表示需要关闭连接。之后主动关闭方进入 FIN_WAIT_1 状态。</p>
<p>接着，接收到这个 FIN 包的对端执行被动关闭。这个 FIN 由 TCP 协议栈处理，我们知道，TCP 协议栈为 FIN 包插入一个文件结束符 EOF 到接收缓冲区中，应用程序可以通过 read 调用来感知这个 FIN 包。一定要注意，这个 EOF 会被放在已排队等候的其他已接收的数据之后，这就意味着接收端应用程序需要处理这种异常情况，因为 EOF 表示在该连接上再无额外数据到达。此时，被动关闭方进入 CLOSE_WAIT 状态。</p>
<p>接下来，被动关闭方将读到这个 EOF，于是，应用程序也调用 close 关闭它的套接字，这导致它的 TCP 也发送一个 FIN 包。这样，被动关闭方将进入 LAST_ACK 状态。</p>
<p>最终，主动关闭方接收到对方的 FIN 包，并确认这个 FIN 包。主动关闭方进入 TIME_WAIT 状态，而接收到 ACK 的被动关闭方则进入 CLOSED 状态。进过 2MSL 时间之后，主动关闭方也进入 CLOSED 状态。</p>
<p>你可以看到，每个方向都需要一个 FIN 和一个 ACK，因此通常被称为四次挥手。</p>
<p>当然，这中间使用 shutdown，执行一端到另一端的半关闭也是可以的。</p>
<p>当套接字被关闭时，TCP 为其所在端发送一个 FIN 包。在大多数情况下，这是由应用进程调用 close 而发生的，值得注意的是，一个进程无论是正常退出（exit 或者 main 函数返回），还是非正常退出（比如，收到 SIGKILL 信号关闭，就是我们常常干的 kill -9），所有该进程打开的描述符都会被系统关闭，这也导致 TCP 描述符对应的连接上发出一个 FIN 包。</p>
<p>无论是客户端还是服务器，任何一端都可以发起主动关闭。大多数真实情况是客户端执行主动关闭，你可能不会想到的是，HTTP/1.0 却是由服务器发起主动关闭的。</p>
<h2 id="最大分组-msl-是-tcp-分组在网络中存活的最长时间吗">最大分组 MSL 是 TCP 分组在网络中存活的最长时间吗？</h2>
<p>MSL 是任何 IP 数据报能够在因特网中存活的最长时间。其实它的实现不是靠计时器来完成的，在每个数据报里都包含有一个被称为 TTL（time to live）的 8 位字段，它的最大值为 255。TTL 可译为“生存时间”，这个生存时间由源主机设置初始值，它表示的是一个 IP 数据报可以经过的最大跳跃数，每经过一个路由器，就相当于经过了一跳，它的值就减 1，当此值减为 0 时，则所在的路由器会将其丢弃，同时发送 ICMP 报文通知源主机。RFC793 中规定 MSL 的时间为 2 分钟，Linux 实际设置为 30 秒。</p>
<h2 id="关于-listen-函数中参数-backlog-的释义问题">关于 listen 函数中参数 backlog 的释义问题</h2>
<p>我们该如何理解 listen 函数中的参数 backlog？如果 backlog 表示的是未完成连接队列的大小，那么已完成连接的队列的大小有限制吗？如果都是已经建立连接的状态，那么并发取决于已完成连接的队列的大小吗？</p>
<p>backlog 的值含义从来就没有被严格定义过。原先 Linux 实现中，backlog 参数定义了该套接字对应的未完成连接队列的最大长度 （pending connections)。如果一个连接到达时，该队列已满，客户端将会接收一个 ECONNREFUSED 的错误信息，如果支持重传，该请求可能会被忽略，之后会进行一次重传。</p>
<p>从 Linux 2.2 开始，backlog 的参数内核有了新的语义，它现在定义的是已完成连接队列的最大长度，表示的是已建立的连接（established connection），正在等待被接收（accept 调用返回），而不是原先的未完成队列的最大长度。现在，未完成队列的最大长度值可以通过 /proc/sys/net/ipv4/tcp_max_syn_backlog 完成修改，默认值为 128。</p>
<p>至于已完成连接队列，如果声明的 backlog 参数比 /proc/sys/net/core/somaxconn 的参数要大，那么就会使用我们声明的那个值。实际上，这个默认的值为 128。注意在 Linux 2.4.25 之前，这个值是不可以修改的一个固定值，大小也是 128。</p>
<p>设计良好的程序，在 128 固定值的情况下也是可以支持成千上万的并发连接的，这取决于 I/O 分发的效率，以及多线程程序的设计。</p>
<h2 id="udp-连接和断开套接字的过程是怎样的">UDP 连接和断开套接字的过程是怎样的？</h2>
<p>UDP 连接套接字不是发起连接请求的过程，而是记录目的地址和端口到套接字的映射关系。</p>
<p>断开套接字则相反，将删除原来记录的映射关系。</p>
<h1 id="如何同时感知多个io事件">如何同时感知多个I/O事件</h1>
<h2 id="什么是-io-多路复用">什么是 I/O 多路复用</h2>
<p>我们可以把标准输入、套接字等都看做 I/O 的一路，多路复用的意思，就是在任何一路 I/O 有“事件”发生的情况下，通知应用程序去处理相应的 I/O 事件，这样我们的程序就变成了“多面手”，在同一时刻仿佛可以处理多个 I/O 事件。</p>
<p>使用 I/O 复用以后，如果标准输入有数据，立即从标准输入读入数据，通过套接字发送出去；如果套接字有数据可以读，立即可以读出数据。</p>
<p>select 函数就是这样一种常见的 I/O 多路复用技术，通知内核挂起进程，当一个或多个 I/O 事件发生后，控制权返还给应用程序，由应用程序进行 I/O 事件的处理。</p>
<p>这些 I/O 事件的类型非常多，比如：</p>
<p>标准输入文件描述符准备好可以读。<br>
监听套接字准备好，新的连接已经建立成功。<br>
已连接套接字准备好可以写。<br>
如果一个 I/O 事件等待超过了 10 秒，发生了超时事件。</p>
<h2 id="select-函数的使用方法">select 函数的使用方法</h2>
<h2 id="套接字描述符就绪条件">套接字描述符就绪条件</h2>
<h2 id="poll另一种io多路复用">poll：另一种I/O多路复用</h2>
<p>select 方法是多个 UNIX 平台支持的非常常见的 I/O 多路复用技术，它通过描述符集合来表示检测的 I/O 对象，通过三个不同的描述符集合来描述 I/O 事件 ：可读、可写和异常。但是 select 有一个缺点，那就是所支持的文件描述符的个数是有限的。在 Linux 系统中，select 的默认最大值为 1024。</p>
<p>poll 是除了 select 之外，另一种普遍使用的 I/O 多路复用技术，和 select 相比，它和内核交互的数据结构有所变化，另外，也突破了文件描述符的个数限制。</p>
<h3 id="poll-函数介绍">poll 函数介绍</h3>
<h1 id="非阻塞io">非阻塞I/O</h1>
<h2 id="阻塞-vs-非阻塞">阻塞 VS 非阻塞</h2>
<p>当应用程序调用阻塞 I/O 完成某个操作时，应用程序会被挂起，等待内核完成操作，感觉上应用程序像是被“阻塞”了一样。实际上，内核所做的事情是将 CPU 时间切换给其他有需要的进程，网络应用程序在这种情况下就会得不到 CPU 时间做该做的事情。</p>
<p>非阻塞 I/O 则不然，当应用程序调用非阻塞 I/O 完成某个操作时，内核立即返回，不会把 CPU 时间切换给其他进程，应用程序在返回后，可以得到足够的 CPU 时间继续完成其他事情。</p>
<h3 id="非阻塞-io">非阻塞 I/O</h3>
<h4 id="读操作">读操作</h4>
<p>如果套接字对应的接收缓冲区没有数据可读，在非阻塞情况下 read 调用会立即返回，一般返回 EWOULDBLOCK 或 EAGAIN 出错信息。在这种情况下，出错信息是需要小心处理，比如后面再次调用 read 操作，而不是直接作为错误直接返回。需要不断进行又一次轮询处理。</p>
<h4 id="写操作">写操作</h4>
<p>在非阻塞 I/O 的情况下，如果套接字的发送缓冲区已达到了极限，不能容纳更多的字节，那么操作系统内核会尽最大可能从应用程序拷贝数据到发送缓冲区中，并立即从 write 等函数调用中返回。可想而知，在拷贝动作发生的瞬间，有可能一个字符也没拷贝，有可能所有请求字符都被拷贝完成，那么这个时候就需要返回一个数值，告诉应用程序到底有多少数据被成功拷贝到了发送缓冲区中，应用程序需要再次调用 write 函数，以输出未完成拷贝的字节。</p>
<p>write 等函数是可以同时作用到阻塞 I/O 和非阻塞 I/O 上的，为了复用一个函数，处理非阻塞和阻塞 I/O 多种情况，设计出了写入返回值，并用这个返回值表示实际写入的数据大小。</p>
<p>也就是说，非阻塞 I/O 和阻塞 I/O 处理的方式是不一样的。</p>
<p>非阻塞 I/O 需要这样：拷贝→返回→再拷贝→再返回。</p>
<p>而阻塞 I/O 需要这样：拷贝→直到所有数据拷贝至发送缓冲区完成→返回。<br>
<img src="https://cc1024201.github.io/post-images/1623661504246.png" alt="" loading="lazy"></p>
<ol>
<li>read 总是在接收缓冲区有数据时就立即返回，不是等到应用程序给定的数据充满才返回。当接收缓冲区为空时，阻塞模式会等待，非阻塞模式立即返回 -1，并有 EWOULDBLOCK 或 EAGAIN 错误。</li>
<li>和 read 不同，阻塞模式下，write 只有在发送缓冲区足以容纳应用程序的输出字节时才返回；而非阻塞模式下，则是能写入多少就写入多少，并返回实际写入的字节数。</li>
<li>阻塞模式下的 write 有个特例, 就是对方主动关闭了套接字，这个时候 write 调用会立即返回，并通过返回值告诉应用程序实际写入的字节数，如果再次对这样的套接字进行 write 操作，就会返回失败。失败是通过返回值 -1 来通知到应用程序的。</li>
</ol>
<h4 id="accept-2">accept</h4>
<p>当 accept 和 I/O 多路复用 select、poll 等一起配合使用时，如果在监听套接字上触发事件，说明有连接建立完成，此时调用 accept 肯定可以返回已连接套接字。</p>
<h1 id="epoll">epoll</h1>
<p>本质上 epoll 还是一种 I/O 多路复用技术， epoll 通过监控注册的多个描述字，来进行 I/O 事件的分发处理。不同于 poll 的是，epoll 不仅提供了默认的 level-triggered（条件触发）机制，还提供了性能更为强劲的 edge-triggered（边缘触发）机制。<br>
使用 epoll 进行网络程序的编写，需要三个步骤，分别是 epoll_create，epoll_ctl 和 epoll_wait。</p>
<h2 id="epoll_create">epoll_create</h2>
<h2 id="epoll_ctl">epoll_ctl</h2>
<h2 id="epoll_wait">epoll_wait</h2>
<h2 id="edge-triggered-vs-level-triggered">edge-triggered VS level-triggered</h2>
<h2 id="总结-9">总结</h2>
<p>epoll 通过改进的接口设计，避免了用户态 - 内核态频繁的数据拷贝，大大提高了系统性能。在使用 epoll 的时候，我们一定要理解条件触发和边缘触发两种模式。条件触发的意思是只要满足事件的条件，比如有数据需要读，就一直不断地把这个事件传递给用户；而边缘触发的意思是只有第一次满足条件的时候才触发，之后就不会再传递同样的事件了。</p>
<h1 id="高并发模型设计">高并发模型设计</h1>
<h2 id="c10k-问题">C10K 问题</h2>
<p>C10K 问题是这样的：如何在一台物理机上同时服务 10000 个用户？这里 C 表示并发，10K 等于 10000。</p>
<h2 id="操作系统层面">操作系统层面</h2>
<p>C10K 问题本质上是一个操作系统问题，要在一台主机上同时支持 1 万个连接，意味着什么呢? 需要考虑哪些方面？</p>
<h3 id="文件句柄">文件句柄</h3>
<p>首先，通过前面的介绍，我们知道每个客户连接都代表一个文件描述符，一旦文件描述符不够用了，新的连接就会被放弃<br>
在 Linux 下，单个进程打开的文件句柄数是有限制的，没有经过修改的值一般都是 1024。</p>
<h3 id="系统内存">系统内存</h3>
<p>每个 TCP 连接都需要占用一定的发送缓冲区和接收缓冲区。</p>
<h3 id="网络带宽">网络带宽</h3>
<h2 id="c10k-问题解决之道">C10K 问题解决之道</h2>
<p>在网络编程中，涉及到频繁的用户态 - 内核态数据拷贝，设计不够好的程序可能在低并发的情况下工作良好，一旦到了高并发情形，其性能可能呈现出指数级别的损失。</p>
<h3 id="阻塞-io-进程">阻塞 I/O + 进程</h3>
<p>这种方式最为简单直接，每个连接通过 fork 派生一个子进程进行处理，因为一个独立的子进程负责处理了该连接所有的 I/O，所以即便是阻塞 I/O，多个连接之间也不会互相影响。</p>
<p>这个方法虽然简单，但是效率不高，扩展性差，资源占用率高。</p>
<h3 id="阻塞-io-线程">阻塞 I/O + 线程</h3>
<p>通过为每个连接调用 pthread_create 创建一个单独的线程，也可以达到上面使用进程的效果。<br>
因为线程的创建是比较消耗资源的，况且不是每个连接在每个时刻都需要服务，因此，我们可以预先通过创建一个线程池，并在多个连接中复用线程池来获得某种效率上的提升。</p>
<h3 id="非阻塞-io-readiness-notification-单线程">非阻塞 I/O + readiness notification + 单线程</h3>
<p>应用程序其实可以采取轮询的方式来对保存的套接字集合进行挨个询问，从而找出需要进行 I/O 处理的套接字<br>
但这个方法有一个问题，如果套接字有一万个之多，每次循环判断都会消耗大量的 CPU 时间，而且极有可能在一个循环之内，没有任何一个套接字准备好可读，或者可写。</p>
<p>既然这样，CPU 的消耗太大，那么干脆让操作系统来告诉我们哪个套接字可以读，哪个套接字可以写。在这个结果发生之前，我们把 CPU 的控制权交出去，让操作系统来把宝贵的 CPU 时间调度给那些需要的进程，这就是 select、poll 这样的 I/O 分发技术。</p>
<p>但是，这样的方法需要每次 dispatch 之后，对所有注册的套接字进行逐个排查，效率并不是最高的。如果 dispatch 调用返回之后只提供有 I/O 事件或者 I/O 变化的套接字，这样排查的效率不就高很多了么？这就是前面我们讲到的 epoll 设计。</p>
<h3 id="非阻塞-io-readiness-notification-多线程">非阻塞 I/O + readiness notification + 多线程</h3>
<p>前面的做法是所有的 I/O 事件都在一个线程里分发，如果我们把线程引入进来，可以利用现代 CPU 多核的能力，让每个核都可以作为一个 I/O 分发器进行 I/O 事件的分发。</p>
<p>这就是所谓的主从 reactor 模式。基于 epoll/poll/select 的 I/O 事件分发器可以叫做 reactor，也可以叫做事件驱动，或者事件轮询（eventloop）。</p>
<h3 id="异步-io-多线程">异步 I/O+ 多线程</h3>
<p>异步非阻塞 I/O 模型是一种更为高效的方式，当调用结束之后，请求立即返回，由操作系统后台完成对应的操作，当最终操作完成，就会产生一个信号，或者执行一个回调函数来完成 I/O 处理。（这就涉及到了 Linux 下的 aio 机制）</p>
<h1 id="使用阻塞io和进程模型最传统的方式">使用阻塞I/O和进程模型：最传统的方式</h1>
<h2 id="父进程和子进程">父进程和子进程</h2>
<p>我们知道，进程是程序执行的最小单位，一个进程有完整的地址空间、程序计数器等，如果想创建一个新的进程，使用函数 fork 就可以。</p>
<h3 id="僵尸进程">僵尸进程</h3>
<h2 id="阻塞-io-的进程模型">阻塞 I/O 的进程模型</h2>
<p><img src="https://cc1024201.github.io/post-images/1623662645670.png" alt="" loading="lazy"><br>
<img src="https://cc1024201.github.io/post-images/1623662649921.png" alt="" loading="lazy"></p>
<h1 id="使用阻塞io和线程模型换一种轻量的方式">使用阻塞I/O和线程模型：换一种轻量的方式</h1>
<h2 id="posix-线程模型">POSIX 线程模型</h2>
<p>POSIX 线程是现代 UNIX 系统提供的处理线程的标准接口。POSIX 定义的线程函数大约有 60 多个，这些函数可以帮助我们创建线程、回收线程。</p>
<h2 id="主要线程函数">主要线程函数</h2>
<h3 id="创建线程">创建线程</h3>
<h3 id="终止线程">终止线程</h3>
<h3 id="回收已终止线程的资源">回收已终止线程的资源</h3>
<h3 id="分离线程">分离线程</h3>
<h2 id="每个连接一个线程处理">每个连接一个线程处理</h2>
<h2 id="构建线程池处理多个连接">构建线程池处理多个连接</h2>
<p>我们可以使用预创建线程池的方式来进行优化。在服务器端启动时，可以先按照固定大小预创建出多个线程，当有新连接建立时，往连接字队列里放置这个新连接描述字，线程池里的线程负责从连接字队列里取出连接描述字进行处理。<br>
<img src="https://cc1024201.github.io/post-images/1623662847693.png" alt="" loading="lazy"><br>
因为线程池大小固定，又因为使用了阻塞套接字，肯定会出现有连接得不到及时服务的场景。这个问题的解决还是要回到我在开篇词里提到的方案上来，多路 I/O 复用加上线程来处理，仅仅使用阻塞 I/O 模型和线程是没有办法达到极致的高并发处理能力。</p>
<h1 id="使用poll单线程处理所有io事件">使用poll单线程处理所有I/O事件</h1>
<h2 id="重温事件驱动">重温事件驱动</h2>
<h3 id="基于事件的程序设计-gui-web">基于事件的程序设计: GUI、Web</h3>
<p>事件驱动的好处是占用资源少，效率高，可扩展性强，是支持高性能高并发的不二之选。<br>
事件驱动模型，也被叫做反应堆模型（reactor），或者是 Event loop 模型。这个模型的核心有两点。</p>
<p>第一，它存在一个无限循环的事件分发线程，或者叫做 reactor 线程、Event loop 线程。这个事件分发线程的背后，就是 poll、epoll 等 I/O 分发技术的使用。</p>
<p>第二，所有的 I/O 操作都可以抽象成事件，每个事件必须有回调函数来处理。acceptor 上有连接建立成功、已连接套接字上发送缓冲区空出可以写、通信管道 pipe 上有数据可以读，这些都是一个个事件，通过事件分发，这些事件都可以一一被检测，并调用对应的回调函数加以处理。</p>
<h2 id="几种-io-模型和线程模型设计">几种 I/O 模型和线程模型设计</h2>
<p>任何一个网络程序，所做的事情可以总结成下面几种：</p>
<p>read：从套接字收取数据；<br>
decode：对收到的数据进行解析；<br>
compute：根据解析之后的内容，进行计算和处理；<br>
encode：将处理之后的结果，按照约定的格式进行编码；<br>
send：最后，通过套接字把结果发送出去。<br>
这几个过程和套接字最相关的是 read 和 send 这两种。</p>
<h3 id="fork">fork</h3>
<figure data-type="image" tabindex="4"><img src="https://cc1024201.github.io/post-images/1623663342027.png" alt="" loading="lazy"></figure>
<p>随着客户数的变多，fork 的子进程也越来越多，即使客户和服务器之间的交互比较少，这样的子进程也不能被销毁，一直需要存在。使用 fork 的方式处理非常简单，它的缺点是处理效率不高，fork 子进程的开销太大。</p>
<h3 id="pthread">pthread</h3>
<p><img src="https://cc1024201.github.io/post-images/1623663378209.png" alt="" loading="lazy"><br>
因为线程是比进程更轻量级的执行单位，所以它的效率相比 fork 的方式，有一定的提高。但是，每次创建一个线程的开销仍然是不小的，因此，引入了线程池的概念，预先创建出一个线程池，在每次新连接达到时，从线程池挑选出一个线程为之服务，很好地解决了线程创建的开销。但是，这个模式还是没有解决空闲连接占用资源的问题，如果一个连接在一定时间内没有数据交互，这个连接还是要占用一定的线程资源，直到这个连接消亡为止。</p>
<h3 id="single-reactor-thread">single reactor thread</h3>
<p><img src="https://cc1024201.github.io/post-images/1623663428953.png" alt="" loading="lazy"><br>
一个 reactor 线程上同时负责分发 acceptor 的事件、已连接套接字的 I/O 事件。</p>
<h3 id="single-reactor-thread-worker-threads">single reactor thread + worker threads</h3>
<p><img src="https://cc1024201.github.io/post-images/1623663527367.png" alt="" loading="lazy"><br>
一个 reactor 线程有一个问题，和 I/O 事件处理相比，应用程序的业务逻辑处理是比较耗时的，比如 XML 文件的解析、数据库记录的查找、文件资料的读取和传输、计算型工作的处理等，这些工作相对而言比较独立，它们会拖慢整个反应堆模式的执行效率。<br>
所以，将这些 decode、compute、enode 型工作放置到另外的线程池中，和反应堆线程解耦，是一个比较明智的选择。反应堆线程只负责处理 I/O 相关的工作，业务逻辑相关的工作都被裁剪成一个一个的小任务，放到线程池里由空闲的线程来执行。当结果完成后，再交给反应堆线程，由反应堆线程通过套接字将结果发送出去。</p>
<h1 id="io多路复用进阶子线程使用poll处理连接io事件">I/O多路复用进阶：子线程使用poll处理连接I/O事件</h1>
<p>单 reactor 线程既分发连接建立，又分发已建立连接的 I/O，有点忙不过来，在实战中的表现可能就是客户端连接成功率偏低。<br>
将 acceptor 上的连接建立事件和已建立连接的 I/O 事件分离，形成所谓的主 - 从 reactor 模式。</p>
<h2 id="主-从-reactor-模式">主 - 从 reactor 模式</h2>
<figure data-type="image" tabindex="5"><img src="https://cc1024201.github.io/post-images/1623663679883.png" alt="" loading="lazy"></figure>
<p>主 - 从这个模式的核心思想是，主反应堆线程只负责分发 Acceptor 连接建立，已连接套接字上的 I/O 事件交给 sub-reactor 负责分发。其中 sub-reactor 的数量，可以根据 CPU 的核数来灵活设置。</p>
<p>比如一个四核 CPU，我们可以设置 sub-reactor 为 4。相当于有 4 个身手不凡的反应堆线程同时在工作，这大大增强了 I/O 分发处理的效率。而且，同一个套接字事件分发只会出现在一个反应堆线程中，这会大大减少并发处理的锁开销。</p>
<p>我们的主反应堆线程一直在感知连接建立的事件，如果有连接成功建立，主反应堆线程通过 accept 方法获取已连接套接字，接下来会按照一定的算法选取一个从反应堆线程，并把已连接套接字加入到选择好的从反应堆线程中。</p>
<h2 id="主-从-reactorworker-threads-模式">主 - 从 reactor+worker threads 模式</h2>
<p><img src="https://cc1024201.github.io/post-images/1623665045601.png" alt="" loading="lazy"><br>
如果说主 - 从 reactor 模式解决了 I/O 分发的高效率问题，那么 work threads 就解决了业务逻辑和 I/O 分发之间的耦合问题。把这两个策略组装在一起，就是实战中普遍采用的模式。大名鼎鼎的 Netty，就是把这种模式发挥到极致的一种实现。不过要注意 Netty 里面提到的 worker 线程，其实就是我们这里说的从 reactor 线程，并不是处理具体业务逻辑的 worker 线程。</p>
<p>下面贴的一段代码就是常见的 Netty 初始化代码，这里 Boss Group 就是 acceptor 主反应堆，workerGroup 就是从反应堆。而处理业务逻辑的线程，通常都是通过使用 Netty 的程序开发者进行设计和定制，一般来说，业务逻辑线程需要从 workerGroup 线程中分离，以便支持更高的并发度。</p>
<pre><code class="language-java">public final class TelnetServer {
    static final int PORT = Integer.parseInt(System.getProperty(&quot;port&quot;, SSL? &quot;8992&quot; : &quot;8023&quot;));
 
    public static void main(String[] args) throws Exception {
        // 产生一个 reactor 线程，只负责 accetpor 的对应处理
        EventLoopGroup bossGroup = new NioEventLoopGroup(1);
        // 产生一个 reactor 线程，负责处理已连接套接字的 I/O 事件分发
        EventLoopGroup workerGroup = new NioEventLoopGroup(1);
        try {
           // 标准的 Netty 初始，通过 serverbootstrap 完成线程池、channel 以及对应的 handler 设置，注意这里讲 bossGroup 和 workerGroup 作为参数设置
            ServerBootstrap b = new ServerBootstrap();
            b.group(bossGroup, workerGroup)
             .channel(NioServerSocketChannel.class)
             .handler(new LoggingHandler(LogLevel.INFO))
             .childHandler(new TelnetServerInitializer(sslCtx));
 
            // 开启两个 reactor 线程无限循环处理
            b.bind(PORT).sync().channel().closeFuture().sync();
        } finally {
            bossGroup.shutdownGracefully();
            workerGroup.shutdownGracefully();
        }
    }
}
</code></pre>
<h1 id="使用epoll和多线程模型">使用epoll和多线程模型</h1>
<figure data-type="image" tabindex="6"><img src="https://cc1024201.github.io/post-images/1623665207965.png" alt="" loading="lazy"></figure>
<p>其中主线程的 epoll_wait 只处理 acceptor 套接字的事件，表示的是连接的建立；反应堆子线程的 epoll_wait 主要处理的是已连接套接字的读写事件。</p>
<h2 id="epoll-的性能分析">epoll 的性能分析</h2>
<h2 id="总结-10">总结</h2>
<p>将程序框架切换到了 epoll 的版本，和 poll 版本相比，只是底层的框架做了更改，上层应用程序不用做任何修改，这也是程序框架强大的地方。和 poll 相比，epoll 从事件集合和就绪列表两个方面加强了程序性能，是 Linux 下高性能网络程序的首选。</p>
<h1 id="异步io探索">异步I/O探索</h1>
<h2 id="阻塞-非阻塞-vs-同步-异步">阻塞 / 非阻塞 VS 同步 / 异步</h2>
<p><img src="https://cc1024201.github.io/post-images/1623665380196.png" alt="" loading="lazy"><br>
第一种是阻塞 I/O。阻塞 I/O 发起的 read 请求，线程会被挂起，一直等到内核数据准备好，并把数据从内核区域拷贝到应用程序的缓冲区中，当拷贝过程完成，read 请求调用才返回。接下来，应用程序就可以对缓冲区的数据进行数据解析。</p>
<p><img src="https://cc1024201.github.io/post-images/1623665393642.png" alt="" loading="lazy"><br>
第二种是非阻塞 I/O。非阻塞的 read 请求在数据未准备好的情况下立即返回，应用程序可以不断轮询内核，直到数据准备好，内核将数据拷贝到应用程序缓冲，并完成这次 read 调用。注意，这里最后一次 read 调用，获取数据的过程，是一个同步的过程。这里的同步指的是内核区域的数据拷贝到缓存区这个过程。<br>
每次让应用程序去轮询内核的 I/O 是否准备好，是一个不经济的做法，因为在轮询的过程中应用进程啥也不能干。于是，像 select、poll 这样的 I/O 多路复用技术就隆重登场了。通过 I/O 事件分发，当内核数据准备好时，再通知应用程序进行操作。这个做法大大改善了应用进程对 CPU 的利用率，在没有被通知的情况下，应用进程可以使用 CPU 做其他的事情。</p>
<p><img src="https://cc1024201.github.io/post-images/1623665526473.png" alt="" loading="lazy"><br>
这里 read 调用，获取数据的过程，也是一个同步的过程。<br>
第一种阻塞 I/O 我想你已经比较了解了，在阻塞 I/O 的情况下，应用程序会被挂起，直到获取数据。第二种非阻塞 I/O 和第三种基于非阻塞 I/O 的多路复用技术，获取数据的操作不会被阻塞。</p>
<p>无论是第一种阻塞 I/O，还是第二种非阻塞 I/O，第三种基于非阻塞 I/O 的多路复用都是同步调用技术。为什么这么说呢？因为同步调用、异步调用的说法，是对于获取数据的过程而言的，前面几种最后获取数据的 read 操作调用，都是同步的，在 read 调用时，内核将数据从内核空间拷贝到应用程序空间，这个过程是在 read 函数中同步进行的，如果内核实现的拷贝效率很差，read 调用就会在这个同步过程中消耗比较长的时间。</p>
<p><img src="https://cc1024201.github.io/post-images/1623665559461.png" alt="" loading="lazy"><br>
第四种 I/O 技术，当我们发起 aio_read 之后，就立即返回，内核自动将数据从内核空间拷贝到应用程序空间，这个拷贝过程是异步的，内核自动完成的，和前面的同步操作不一样，应用程序并不需要主动发起拷贝动作。</p>
<figure data-type="image" tabindex="7"><img src="https://cc1024201.github.io/post-images/1623665591738.png" alt="" loading="lazy"></figure>
<p>Linux 下对异步操作的支持非常有限，这也是为什么使用 epoll 等多路分发技术加上非阻塞 I/O 来解决 Linux 下高并发高性能网络 I/O 问题的根本原因。</p>
<h2 id="windows-下的-iocp-和-proactor-模式">Windows 下的 IOCP 和 Proactor 模式</h2>
<h1 id="epoll源码深度剖析">epoll源码深度剖析</h1>
<h2 id="基本数据结构">基本数据结构</h2>
<h2 id="epoll_create-2">epoll_create</h2>
<h2 id="epoll_ctl-2">epoll_ctl</h2>
<h2 id="查找-epoll-实例">查找 epoll 实例</h2>
<h2 id="红黑树查找">红黑树查找</h2>
<h2 id="ep_insert">ep_insert</h2>
<h2 id="ep_poll_callback">ep_poll_callback</h2>
<h2 id="ep_poll">ep_poll</h2>
<h2 id="ep_send_events">ep_send_events</h2>
<h2 id="level-triggered-vs-edge-triggered">Level-triggered VS Edge-triggered</h2>
<h2 id="epoll-vs-pollselect">epoll VS poll/select</h2>
<h1 id="高性能http服务器一设计和思路">高性能HTTP服务器（一）：设计和思路</h1>
<p>在开始编写高性能 HTTP 服务器之前，我们先要构建一个支持 TCP 的高性能网络编程框架，完成这个 TCP 高性能网络框架之后，再增加 HTTP 特性的支持就比较容易了，这样就可以很快开发出一个高性能的 HTTP 服务器程序。</p>
<h2 id="设计需求">设计需求</h2>
<p>TCP 高性能网络框架需要满足的需求有以下三点。</p>
<p>第一，采用 reactor 模型，可以灵活使用 poll/epoll 作为事件分发实现。</p>
<p>第二，必须支持多线程，从而可以支持单线程单 reactor 模式，也可以支持多线程主 - 从 reactor 模式。可以将套接字上的 I/O 事件分离到多个线程上。</p>
<p>第三，封装读写操作到 Buffer 对象中。</p>
<p>按照这三个需求，正好可以把整体设计思路分成三块来讲解，分别包括反应堆模式设计、I/O 模型和多线程模型设计、数据读写封装和 buffer。</p>
<h2 id="主要设计思路">主要设计思路</h2>
<h3 id="反应堆模式设计">反应堆模式设计</h3>
<h4 id="event_loop">event_loop</h4>
<p>你可以把 event_loop 这个对象理解成和一个线程绑定的无限事件循环，你会在各种语言里看到 event_loop 这个抽象。这是什么意思呢？简单来说，它就是一个无限循环着的事件分发器，一旦有事件发生，它就会回调预先定义好的回调函数，完成事件的处理。</p>
<p>具体来说，event_loop 使用 poll 或者 epoll 方法将一个线程阻塞，等待各种 I/O 事件的发生。</p>
<h4 id="channel">channel</h4>
<p>对各种注册到 event_loop 上的对象，我们抽象成 channel 来表示，例如注册到 event_loop 上的监听事件，注册到 event_loop 上的套接字读写事件等。在各种语言的 API 里，你都会看到 channel 这个对象</p>
<h4 id="acceptor">acceptor</h4>
<p>acceptor 对象表示的是服务器端监听器，acceptor 对象最终会作为一个 channel 对象，注册到 event_loop 上，以便进行连接完成的事件分发和检测。</p>
<h4 id="event_dispatcher">event_dispatcher</h4>
<p>event_dispatcher 是对事件分发机制的一种抽象，也就是说，可以实现一个基于 poll 的 poll_dispatcher，也可以实现一个基于 epoll 的 epoll_dispatcher。在这里，我们统一设计一个 event_dispatcher 结构体，来抽象这些行为。</p>
<h4 id="channel_map">channel_map</h4>
<p>channel_map 保存了描述字到 channel 的映射，这样就可以在事件发生时，根据事件类型对应的套接字快速找到 chanel 对象里的事件处理函数。</p>
<h3 id="io-模型和多线程模型设计">I/O 模型和多线程模型设计</h3>
<p>I/O 线程和多线程模型，主要解决 event_loop 的线程运行问题，以及事件分发和回调的线程执行问题。</p>
<h4 id="thread_pool">thread_pool</h4>
<p>thread_pool 维护了一个 sub-reactor 的线程列表，它可以提供给主 reactor 线程使用，每次当有新的连接建立时，可以从 thread_pool 里获取一个线程，以便用它来完成对新连接套接字的 read/write 事件注册，将 I/O 线程和主 reactor 线程分离。</p>
<h4 id="event_loop_thread">event_loop_thread</h4>
<p>event_loop_thread 是 reactor 的线程实现，连接套接字的 read/write 事件检测都是在这个线程里完成的。</p>
<h3 id="buffer-和数据读写">Buffer 和数据读写</h3>
<h4 id="buffer">buffer</h4>
<p>buffer 对象屏蔽了对套接字进行的写和读的操作，如果没有 buffer 对象，连接套接字的 read/write 事件都需要和字节流直接打交道，这显然是不友好的。所以，我们也提供了一个基本的 buffer 对象，用来表示从连接套接字收取的数据，以及应用程序即将需要发送出去的数据。</p>
<h4 id="tcp_connection">tcp_connection</h4>
<p>tcp_connection 这个对象描述的是已建立的 TCP 连接。它的属性包括接收缓冲区、发送缓冲区、channel 对象等。这些都是一个 TCP 连接的天然属性。</p>
<p>tcp_connection 是大部分应用程序和我们的高性能框架直接打交道的数据结构。我们不想把最下层的 channel 对象暴露给应用程序，因为抽象的 channel 对象不仅仅可以表示 tcp_connection，前面提到的监听套接字也是一个 channel 对象，后面提到的唤醒 socketpair 也是一个 channel 对象。所以，我们设计了 tcp_connection 这个对象，希望可以提供给用户比较清晰的编程入口。</p>
<h2 id="反应堆模式设计-2">反应堆模式设计</h2>
<h3 id="概述">概述</h3>
<p><img src="https://cc1024201.github.io/post-images/1623666724657.png" alt="" loading="lazy"><br>
当 event_loop_run 完成之后，线程进入循环，首先执行 dispatch 事件分发，一旦有事件发生，就会调用 channel_event_activate 函数，在这个函数中完成事件回调函数 eventReadcallback 和 eventWritecallback 的调用，最后再进行 event_loop_handle_pending_channel，用来修改当前监听的事件列表，完成这个部分之后，又进入了事件分发循环。</p>
<h3 id="event_loop-分析">event_loop 分析</h3>
<p>说 event_loop 是整个反应堆模式设计的核心，一点也不为过。先看一下 event_loop 的数据结构。</p>
<p>在这个数据结构中，最重要的莫过于 event_dispatcher 对象了。你可以简单地把 event_dispatcher 理解为 poll 或者 epoll，它可以让我们的线程挂起，等待事件的发生。</p>
<h3 id="event_dispacher-分析">event_dispacher 分析</h3>
<p>为了实现不同的事件分发机制，这里把 poll、epoll 等抽象成了一个 event_dispatcher 结构。event_dispatcher 的具体实现有 poll_dispatcher 和 epoll_dispatcher 两种</p>
<h3 id="channel-对象分析">channel 对象分析</h3>
<p>channel 对象是用来和 event_dispather 进行交互的最主要的结构体，它抽象了事件分发。一个 channel 对应一个描述字，描述字上可以有 READ 可读事件，也可以有 WRITE 可写事件。channel 对象绑定了事件处理函数 event_read_callback 和 event_write_callback。</p>
<h3 id="channel_map-对象分析">channel_map 对象分析</h3>
<p>event_dispatcher 在获得活动事件列表之后，需要通过文件描述字找到对应的 channel，从而回调 channel 上的事件处理函数 event_read_callback 和 event_write_callback，为此，设计了 channel_map 对象。</p>
<h3 id="增加-删除-修改-channel-event">增加、删除、修改 channel event</h3>
<h1 id="高性能http服务器二io模型和多线程模型实现">高性能HTTP服务器（二）：I/O模型和多线程模型实现</h1>
<h2 id="多线程设计的几个考虑">多线程设计的几个考虑</h2>
<p>在我们的设计中，main reactor 线程是一个 acceptor 线程，这个线程一旦创建，会以event_loop 形式阻塞在 event_dispatcher 的 dispatch 方法上，实际上，它在等待监听套接字上的事件发生，也就是已完成的连接，一旦有连接完成，就会创建出连接对象tcp_connection，以及 channel 对象等。</p>
<p>当用户期望使用多个 sub-reactor 子线程时，主线程会创建多个子线程，每个子线程在创建之后，按照主线程指定的启动函数立即运行，并进行初始化。随之而来的问题是，主线程如何判断子线程已经完成初始化并启动，继续执行下去呢？这是一个需要解决的重点问题。<br>
在设置了多个线程的情况下，需要将新创建的已连接套接字对应的读写事件交给一个 subreactor线程处理。所以，这里从 thread_pool 中取出一个线程，通知这个线程有新的事件加入。而这个线程很可能是处于事件分发的阻塞调用之中，如何协调主线程数据写入给子线程，这是另一个需要解决的重点问题。<br>
<img src="https://cc1024201.github.io/post-images/1623678351324.png" alt="" loading="lazy"><br>
<img src="https://cc1024201.github.io/post-images/1623678356236.png" alt="" loading="lazy"></p>
<h2 id="主线程等待多个-sub-reactor-子线程初始化完">主线程等待多个 sub-reactor 子线程初始化完</h2>
<p>主线程需要等待子线程完成初始化，也就是需要获取子线程对应数据的反馈，而子线程初始<br>
化也是对这部分数据进行初始化，实际上这是一个多线程的通知问题。</p>
<h2 id="增加已连接套接字事件到-sub-reactor-线程中">增加已连接套接字事件到 sub-reactor 线程中</h2>
<h1 id="高性能http服务器三tcp字节流处理和http协议实现">高性能HTTP服务器（三）：TCP字节流处理和HTTP协议实现</h1>
<h2 id="buffer-对象">buffer 对象</h2>
<p>buffer，顾名思义，就是一个缓冲区对象，缓存了从套接字接收来的数据以及需要发往套接字的数据。<br>
如果是从套接字接收来的数据，事件处理回调函数在不断地往 buffer 对象增加数据，同时，应用程序需要不断把 buffer 对象中的数据处理掉，这样，buffer 对象才可以空出新的位置容纳更多的数据。<br>
如果是发往套接字的数据，应用程序不断地往 buffer 对象增加数据，同时，事件处理回调函数不断调用套接字上的发送函数将数据发送出去，减少 buffer 对象中的写入数据。<br>
可见，buffer 对象是同时可以作为输入缓冲（input buffer）和输出缓冲（output buffer）两个方向使用的，只不过，在两种情形下，写入和读出的对象是有区别的。<br>
<img src="https://cc1024201.github.io/post-images/1623678630460.png" alt="" loading="lazy"></p>
<h2 id="套接字接收数据处理">套接字接收数据处理</h2>
<h2 id="套接字发送数据处理">套接字发送数据处理</h2>
<h2 id="http-协议实现">HTTP 协议实现</h2>
<p>首先定义了一个 http_server 结构，这个 http_server 本质上就是一个TCPServer，只不过暴露给应用程序的回调函数更为简单，只需要看到 http_request 和http_response 结构。</p>
<p>在 http_server 里面，重点是需要完成报文的解析，将解析的报文转化为 http_request 对象，这件事情是通过 http_onMessage 回调函数来完成的。在 http_onMessage 函数里，调用的是 parse_http_request 完成报文解析。</p>
<h2 id="完整的-http-服务器例子">完整的 HTTP 服务器例子</h2>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[关于分析与构建可伸缩的高性能IO服务]]></title>
        <id>https://cc1024201.github.io/post/guan-yu-fen-xi-yu-gou-jian-ke-shen-suo-de-gao-xing-neng-io-fu-wu/</id>
        <link href="https://cc1024201.github.io/post/guan-yu-fen-xi-yu-gou-jian-ke-shen-suo-de-gao-xing-neng-io-fu-wu/">
        </link>
        <updated>2021-06-09T13:35:16.000Z</updated>
        <content type="html"><![CDATA[<p>《Scalable IO in Java》 <a href="http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf">原文连接</a><br>
《Scalable IO in Java》 是java.util.concurrent包的作者，大师Doug Lea关于分析与构建可伸缩的高性能IO服务的一篇经典文章，在文章中Doug Lea通过各个角度，循序渐进的梳理了服务开发中的相关问题，以及在解决问题的过程中服务模型的演变与进化，文章中基于Reactor反应器模式的几种服务模型架构，也被Netty、Mina等大多数高性能IO服务框架所采用，因此阅读这篇文章有助于你更深入了解Netty、Mina等服务框架的编程思想与设计模式。</p>
<h1 id="网络服务">网络服务</h1>
<p>在一般的网络或分布式服务等应用程序中，大都具备一些相同的处理流程</p>
<ol>
<li>读取请求数据；</li>
<li>对请求数据进行解码；</li>
<li>对数据进行处理；</li>
<li>对回复数据进行编码；</li>
<li>发送回复；<br>
当然在实际应用中每一步的运行效率都是不同的，例如其中可能涉及到xml解析、文件传输、web页面的加载、计算服务等不同功能。</li>
</ol>
<h2 id="传统的服务设计模式">传统的服务设计模式</h2>
<p>在一般的网络服务当中都会为每一个连接的处理开启一个新的线程，我们可以看下大致的示意图：<br>
<img src="https://cc1024201.github.io/post-images/1623246452594.png" alt="" loading="lazy"><br>
每一个连接的处理都会对应分配一个新的线程，下面我们看一段经典的Server端Socket服务代码：</p>
<pre><code class="language-java">class Server implements Runnable {
        public void run() {
            try {
                ServerSocket ss = new ServerSocket(PORT);
                while (!Thread.interrupted())
                    new Thread(new Handler(ss.accept())).start();
                // or, single-threaded, or a thread pool
            } catch (IOException ex) {
                /* ... */ 
            }
        }

        static class Handler implements Runnable {
            final Socket socket;

            Handler(Socket s) {
                socket = s;
            }

            public void run() {
                try {
                    byte[] input = new byte[MAX_INPUT];
                    socket.getInputStream().read(input);
                    byte[] output = process(input);
                    socket.getOutputStream().write(output);
                } catch (IOException ex) {
                    /* ... */ 
                }
            }

            private byte[] process(byte[] cmd) {
                /* ... */ 
            }
        }
    }
</code></pre>
<h2 id="构建高性能可伸缩的io服务">构建高性能可伸缩的IO服务</h2>
<p>在构建高性能可伸缩IO服务的过程中，我们希望达到以下的目标：</p>
<p>①　能够在海量负载连接情况下优雅降级；</p>
<p>②　能够随着硬件资源的增加，性能持续改进；</p>
<p>③　具备低延迟、高吞吐量、可调节的服务质量等特点；</p>
<p>而分发处理就是实现上述目标的一个最佳方式。</p>
<h2 id="分发模式">分发模式</h2>
<p>分发模式具有以下几个机制：</p>
<p>①　将一个完整处理过程分解为一个个细小的任务；</p>
<p>②　每个任务执行相关的动作且不产生阻塞；</p>
<p>③　在任务执行状态被触发时才会去执行，例如只在有数据时才会触发读操作；</p>
<p>在一般的服务开发当中，IO事件通常被当做任务执行状态的触发器使用，在hander处理过程中主要针对的也就是IO事件；<br>
<img src="https://cc1024201.github.io/post-images/1623246823006.png" alt="" loading="lazy"><br>
java.nio包就很好的实现了上述的机制：</p>
<p>①　非阻塞的读和写</p>
<p>②　通过感知IO事件分发任务的执行</p>
<p>所以结合一系列基于事件驱动模式的设计，给高性能IO服务的架构与设计带来丰富的可扩展性；</p>
<h1 id="基于事件驱动模式的设计">基于事件驱动模式的设计</h1>
<p>基于事件驱动的架构设计通常比其他架构模型更加有效，因为可以节省一定的性能资源，事件驱动模式下通常不需要为每一个客户端建立一个线程，这意味着更少的线程开销，更少的上下文切换和更少的锁互斥，但任务的调度可能会慢一些，而且通常实现的复杂度也会增加，相关功能必须分解成简单的非阻塞操作，类似与GUI的事件驱动机制，当然也不可能把所有阻塞都消除掉，特别是GC， page faults(内存缺页中断)等。由于是基于事件驱动的，所以需要跟踪服务的相关状态（因为你需要知道什么时候事件会发生）;</p>
<p>下图是AWT中事件驱动设计的一个简单示意图，可以看到，在不同的架构设计中的基于事件驱动的IO操作使用的基本思路是一致的；<br>
<img src="https://cc1024201.github.io/post-images/1623249595506.png" alt="" loading="lazy"></p>
<h1 id="reactor模式">Reactor模式</h1>
<p>Reactor也可以称作反应器模式，它有以下几个特点：</p>
<p>①　Reactor模式中会通过分配适当的handler(处理程序)来响应IO事件，类似与AWT 事件处理线程；</p>
<p>②　每个handler执行非阻塞的操作，类似于AWT ActionListeners 事件监听</p>
<p>③　通过将handler绑定到事件进行管理，类似与AWT addActionListener 添加事件监听；</p>
<h2 id="单线程模式">单线程模式</h2>
<p>下图展示的就是单线程下基本的Reactor设计模式<br>
<img src="https://cc1024201.github.io/post-images/1623249918722.png" alt="" loading="lazy"><br>
首先我们明确下java.nio中相关的几个概念：<br>
Channels<br>
支持非阻塞读写的socket连接；</p>
<p>Buffers<br>
用于被Channels读写的字节数组对象</p>
<p>Selectors<br>
用于判断channle发生IO事件的选择器</p>
<p>SelectionKeys<br>
负责IO事件的状态与绑定</p>
<p>Ok，接下来我们一步步看下基于Reactor模式的服务端设计代码示例：<br>
第一步  Rector线程的初始化</p>
<pre><code class="language-java">class Reactor implements Runnable { 
    final Selector selector;
    final ServerSocketChannel serverSocket;
    Reactor(int port) throws IOException {
        selector = Selector.open();
        serverSocket = ServerSocketChannel.open();
        serverSocket.socket().bind(new InetSocketAddress(port));
        serverSocket.configureBlocking(false);
        SelectionKey sk = serverSocket.register(selector, SelectionKey.OP_ACCEPT); //注册accept事件
        sk.attach(new Acceptor()); //调用Acceptor()为回调方法
    }
    
    public void run() { 
        try {
            while (!Thread.interrupted()) {//循环
                selector.select();
                Set selected = selector.selectedKeys();
                Iterator it = selected.iterator();
                while (it.hasNext())
                    dispatch((SelectionKey)(it.next()); //dispatch分发事件
                selected.clear();
            }
        } catch (IOException ex) { /* ... */ }
    }
    
    void dispatch(SelectionKey k) {
        Runnable r = (Runnable)(k.attachment()); //调用SelectionKey绑定的调用对象
        if (r != null)
            r.run();
    }
    
    // Acceptor 连接处理类
    class Acceptor implements Runnable { // inner
        public void run() {
            try {
                SocketChannel c = serverSocket.accept();
                if (c != null)
                new Handler(selector, c);
            }
            catch(IOException ex) { /* ... */ }
        }
    }
}
</code></pre>
<p>第二步 Handler处理类的初始化</p>
<pre><code class="language-java">final class Handler implements Runnable {
    final SocketChannel socket;
    final SelectionKey sk;
    ByteBuffer input = ByteBuffer.allocate(MAXIN);
    ByteBuffer output = ByteBuffer.allocate(MAXOUT);
    static final int READING = 0, SENDING = 1;
    int state = READING;
    
    Handler(Selector sel, SocketChannel c) throws IOException {
        socket = c;
        c.configureBlocking(false);
        // Optionally try first read now
        sk = socket.register(sel, 0);
        sk.attach(this); //将Handler绑定到SelectionKey上
        sk.interestOps(SelectionKey.OP_READ);
        sel.wakeup();
    }
    boolean inputIsComplete() { /* ... */ }
    boolean outputIsComplete() { /* ... */ }
    void process() { /* ... */ }
    
    public void run() {
        try {
            if (state == READING) read();
            else if (state == SENDING) send();
        } catch (IOException ex) { /* ... */ }
    }
    
    void read() throws IOException {
        socket.read(input);
        if (inputIsComplete()) {
            process();
            state = SENDING;
            // Normally also do first write now
            sk.interestOps(SelectionKey.OP_WRITE);
        }
    }
    void send() throws IOException {
        socket.write(output);
        if (outputIsComplete()) sk.cancel(); 
    }
}
</code></pre>
<p>下面是基于GoF状态对象模式对Handler类的一个优化实现，不需要再进行状态的判断。</p>
<pre><code class="language-java">class Handler { // ...
    public void run() { // initial state is reader
        socket.read(input);
        if (inputIsComplete()) {
            process();
            sk.attach(new Sender()); 
            sk.interest(SelectionKey.OP_WRITE);
            sk.selector().wakeup();
        }
    }
    class Sender implements Runnable {
        public void run(){ // ...
            socket.write(output);
            if (outputIsComplete()) sk.cancel();
        }
    }
}
</code></pre>
<h2 id="多线程设计模式">多线程设计模式</h2>
<p>在多处理器场景下，为实现服务的高性能我们可以有目的的采用多线程模式：</p>
<p>1、增加Worker线程，专门用于处理非IO操作，因为通过上面的程序我们可以看到，反应器线程需要迅速触发处理流程，而如果处理过程也就是process()方法产生阻塞会拖慢反应器线程的性能，所以我们需要把一些非IO操作交给Woker线程来做；</p>
<p>2、拆分并增加反应器Reactor线程，一方面在压力较大时可以饱和处理IO操作，提高处理能力；另一方面维持多个Reactor线程也可以做负载均衡使用；线程的数量可以根据程序本身是CPU密集型还是IO密集型操作来进行合理的分配；</p>
<h3 id="多线程模式">多线程模式</h3>
<p>Reactor多线程设计模式具备以下几个特点：</p>
<p>①　通过卸载非IO操作来提升Reactor 线程的处理性能，这类似与POSA2 中Proactor的设计；</p>
<p>②　比将非IO操作重新设计为事件驱动的方式更简单；</p>
<p>③　但是很难与IO重叠处理，最好能在第一时间将所有输入读入缓冲区；（这里我理解的是最好一次性读取缓冲区数据，方便异步非IO操作处理数据）</p>
<p>④　可以通过线程池的方式对线程进行调优与控制，一般情况下需要的线程数量比客户端数量少很多；<br>
下面是Reactor多线程设计模式的一个示意图与示例代码（我们可以看到在这种模式中在Reactor线程的基础上把非IO操作放在了Worker线程中执行）：<br>
<img src="https://cc1024201.github.io/post-images/1623250351763.png" alt="" loading="lazy"></p>
<pre><code class="language-java">class Handler implements Runnable {
        // uses util.concurrent thread pool
        static PooledExecutor pool = new PooledExecutor(...);//声明线程池
        static final int PROCESSING = 3;

        // ...
        synchronized void read() { // ...
            socket.read(input);
            if (inputIsComplete()) {
                state = PROCESSING;
                pool.execute(new Processer());//处理程序放在线程池中执行
            }
        }

        synchronized void processAndHandOff() {
            process();
            state = SENDING; // or rebind attachment
            sk.interest(SelectionKey.OP_WRITE);
        }

        class Processer implements Runnable {
            public void run() {
                processAndHandOff();
            }
        }
    }
</code></pre>
<p>当你把非IO操作放到线程池中运行时，你需要注意以下几点问题：</p>
<p>①　任务之间的协调与控制，每个任务的启动、执行、传递的速度是很快的，不容易协调与控制；</p>
<p>②　每个hander中dispatch的回调与状态控制；</p>
<p>③　不同线程之间缓冲区的线程安全问题；</p>
<p>④　需要任务返回结果时，任务线程等待和唤醒状态间的切换；</p>
<p>为解决上述问题可以使用PooledExecutor线程池框架，这是一个可控的任务线程池，主函数采用execute(Runnable r)，它具备以下功能，可以很好的对池中的线程与任务进行控制与管理：</p>
<p>①　可设置线程池中最大与最小线程数；</p>
<p>②　按需要判断线程的活动状态，及时处理空闲线程；</p>
<p>③　当执行任务数量超过线程池中线程数量时，有一系列的阻塞、限流的策略；</p>
<h3 id="基于多个反应器的多线程模式">基于多个反应器的多线程模式</h3>
<p>这是对上面模式的进一步完善，使用反应器线程池，一方面根据实际情况用于匹配调节CPU处理与IO读写的效率，提高系统资源的利用率，另一方面在静态或动态构造中每个反应器线程都包含对应的Selector,Thread,dispatchloop,下面是一个简单的代码示例与示意图（Netty就是基于这个模式设计的，一个处理Accpet连接的mainReactor线程，多个处理IO事件的subReactor线程）：</p>
<pre><code class="language-java">Selector[] selectors; // Selector集合，每一个Selector 对应一个subReactor线程
    //mainReactor线程
    class Acceptor { // ...
        public synchronized void run() { 
            //...
            Socket connection = serverSocket.accept(); 
            if (connection != null)
              new Handler(selectors[next], connection); 
            if (++next == selectors.length)
                next = 0;
        }
}
</code></pre>
<p><img src="https://cc1024201.github.io/post-images/1623250776042.png" alt="" loading="lazy"><br>
在服务的设计当中，我们还需要注意与java.nio包特性的结合：</p>
<p>一是注意线程安全，每个selectors 对应一个Reactor 线程，并将不同的处理程序绑定到不同的IO事件，在这里特别需要注意线程之间的同步；</p>
<p>二是java nio中文件传输的方式：</p>
<p>①　Memory-mapped files 内存映射文件的方式，通过缓存区访问文件；</p>
<p>②　Direct buffers直接缓冲区的方式，在合适的情况下可以使用零拷贝传输，但同时这会带来初始化与内存释放的问题（需要池化与主动释放）;</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Netty]]></title>
        <id>https://cc1024201.github.io/post/netty/</id>
        <link href="https://cc1024201.github.io/post/netty/">
        </link>
        <updated>2021-06-09T13:11:26.000Z</updated>
        <content type="html"><![CDATA[<h1 id="介绍">介绍</h1>
<p><a href="https://netty.io/">Netty官网</a><br>
Netty 是一个异步的、基于事件驱动的网络应用框架，用以快速开发高性能、高可靠性的网络 IO 程序。<br>
Netty本质是一个NIO框架，适用于服务器通讯相关的多种应用场景。</p>
<h1 id="应用场景">应用场景</h1>
<p>阿里分布式服务框架 Dubbo 的 Dubbo 协议默认使用 Netty 作为基础通信组件，而其中的RPC调用就是netty+反射。<br>
<img src="https://cc1024201.github.io/post-images/1623244755374.png" alt="" loading="lazy"><br>
经典的 Hadoop 的高性能通信和序列化组件 Avro 的 RPC 框架，默认采用 Netty 进行跨界点通信<br>
<img src="https://cc1024201.github.io/post-images/1623244804578.png" alt="" loading="lazy"></p>
<h1 id="netty-优势">Netty 优势</h1>
<h2 id="原生nio存在的问题">原生NIO存在的问题</h2>
<p>NIO 的类库和 API 繁杂，使用麻烦：需要熟练掌握 Selector、ServerSocketChannel、SocketChannel、ByteBuffer 等。<br>
需要具备其他的额外技能：要熟悉 Java 多线程编程，因为 NIO 编程涉及到 Reactor 模式，你必须对多线程和网络编程非常熟悉，才能编写出高质量的 NIO 程序。<br>
开发工作量和难度都非常大：例如客户端面临断连重连、网络闪断、半包读写、失败缓存、网络拥塞和异常流的处理等等。<br>
JDK NIO 的 Bug：例如Epoll Bug，它会导致 Selector 空轮询，最终导致 CPU 100%。</p>
<h2 id="netty的优点">Netty的优点</h2>
<p>Netty 对 JDK 自带的 NIO 的 API 进行了封装，解决了上述问题。<br>
异步非阻塞通信<br>
Reactor模型优化<br>
高效的并发编程<br>
无锁化的串行设计理念<br>
基于内存池的缓冲区重用机制零拷贝<br>
提供对protobuf等高性能序列化协议支持<br>
设计优雅：适用于各种传输类型的统一 API 阻塞和非阻塞 Socket；基于灵活且可扩展的事件模型，可以清晰地分离关注点；高度可定制的线程模型 - 单线程，一个或多个线程池.<br>
使用方便：详细记录的 Javadoc，用户指南和示例；没有其他依赖项，JDK 5（Netty 3.x）或 6（Netty 4.x）就足够了。<br>
高性能、吞吐量更高：延迟更低；减少资源消耗；最小化不必要的内存复制。<br>
安全：完整的 SSL/TLS 和 StartTLS 支持。<br>
社区活跃、不断更新：发现的 Bug 可以被及时修复，同时，更多的新功能会被加入</p>
<h1 id="netty线程模型">Netty线程模型</h1>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[排序]]></title>
        <id>https://cc1024201.github.io/post/pai-xu/</id>
        <link href="https://cc1024201.github.io/post/pai-xu/">
        </link>
        <updated>2021-06-08T13:48:35.000Z</updated>
        <content type="html"><![CDATA[<h1 id="描述">描述</h1>
<p>给定一个数组，编写一个函数，返回该数组排序后的形式。</p>
<h1 id="示例1">示例1</h1>
<blockquote>
<p>输入：[5,2,3,1,4]<br>
返回值：[1,2,3,4,5]</p>
</blockquote>
<h1 id="示例2">示例2</h1>
<blockquote>
<p>输入：[5,1,6,2,5]<br>
返回值：[1,2,5,5,6]</p>
</blockquote>
<p>排序算法可以分为内部排序和外部排序，内部排序是数据在内存中排序，外部排序是因为数据量很大，一次不能容纳所有记录，在排序过程中需要访问外存。<br>
常见的内部排序算法有：插入排序，希尔排序，选择排序，冒泡排序，归并排序，快速排序，堆排序，基数排序等。<br>
<img src="https://cc1024201.github.io/post-images/1623160428913.png" alt="" loading="lazy"><br>
<img src="https://cc1024201.github.io/post-images/1623160585803.png" alt="" loading="lazy"></p>
<ul>
<li>计算的时间复杂度（最差、平均、和最好性能），依据列表（list）的大小(n)。一般而言，好的性能是O(n log n)（大O符号），坏的性能是O(n<sup>2</sup>)。对于一个排序理想的性能是O(n)，但平均而言不可能达到。基于比较的排序算法对大多数输入而言至少需要O(n log n)。使用具有强大计算能力的计算机，能令时间复杂度趋近于O(n)（但不等于O(n)）。</li>
<li>稳定性：稳定排序算法会让原本有相等键值的纪录维持相对次序。也就是如果一个排序算法是稳定的，当有两个相等键值的纪录R和S，且在原本的列表中R出现在S之前，在排序过的列表中R也将会是在S之前。</li>
</ul>
<h2 id="冒泡排序">冒泡排序</h2>
<p>它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。<br>
冒泡排序是与插入排序拥有相等的运行时间，但是两种算法在需要的交换次数却很大地不同。在最坏的情况，冒泡排序需要O(n<sub>2</sub>)次交换，而插入排序只要最多O(n)交换。冒泡排序的实现（类似下面）通常会对已经排序好的数列拙劣地运行O(n<sub>2</sub>），而插入排序在这个例子只需要O(n)个运算。因此很多现代的算法教科书避免使用冒泡排序，而用插入排序取代之。冒泡排序如果能在内部循环第一次运行时，使用一个旗标来表示有无需要交换的可能，也可以把最优情况下的复杂度降低到O(n)。在这个情况，已经排序好的数列就无交换的需要。若在每次走访数列时，把走访顺序反过来，也可以稍微地改进效率。有时候称为鸡尾酒排序，因为算法会从数列的一端到另一端之间穿梭往返。</p>
<p>冒泡排序算法的运作如下：</p>
<ol>
<li>比较相邻的元素。如果第一个比第二个大，就交换他们两个。</li>
<li>对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。</li>
<li>针对所有的元素重复以上的步骤，除了最后一个。</li>
<li>持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。</li>
</ol>
<ul>
<li>伪代码</li>
</ul>
<pre><code>function bubble_sort (array, length) {
    var i, j;
    for(i from 0 to length-1){
        for(j from 0 to length-1-i){
            if (array[j] &gt; array[j+1])
                swap(array[j], array[j+1])
        }
    }
}
函数 冒泡排序 输入 一个数组名称为array 其长度为length 
    i 从 0 到 (length - 1) 
        j 从 0 到 (length - 1 - i) 
            如果 array[j] &gt; array[j + 1] 
                交换 array[j] 和 array[j + 1] 的值 
            如果结束 
        j循环结束 
    i循环结束 
函数结束
</code></pre>
<pre><code class="language-java">private int[] bubbleSort(int[] array) {
    int temp;
    for (int i = 0; i &lt; array.length - 1; i++) {
        boolean Flag = false; // 是否发生交换。没有交换，提前跳出外层循环
        for (int j = 0; j &lt; array.length - 1 - i; j++) {
            if (array[j] &gt; array[j + 1]) {
                temp = array[j];
                array[j] = array[j + 1];
                array[j + 1] = temp;
                Flag = true;
            }
        }
        if (!Flag)
        {
            break;
        }
    }
    return array;
}
</code></pre>
<h2 id="选择排序">选择排序</h2>
<p>首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。<br>
选择排序的主要优点与数据移动有关。如果某个元素位于正确的最终位置上，则它不会被移动。选择排序每次交换一对元素，它们当中至少有一个将被移到其最终位置上，因此对n个元素的表进行排序总共进行至多(n-1)次交换。在所有的完全依靠交换去移动元素的排序方法中，选择排序属于非常好的一种。</p>
<pre><code class="language-java">public int[] MySort(int[] arr) {
        int temp;
        //总共经过N-1轮比较
        for (int i = 0; i &lt; arr.length - 1; i++) {
            int min = i;
            //每轮需要比较的次数N-i
            for (int j = i + 1; j &lt; arr.length; j++) {
                if (arr[j] &lt; arr[min]) {
                    min = j; //记录当前能找到的最小值元素的下标
                }
            }
            //将找到的最小值和i位置的值进行交换
            if (min != i) {
                temp = arr[i];
                arr[i] = arr[min];
                arr[min] =  temp;

            }
        }
        return arr;
}
</code></pre>
<h2 id="插入排序">插入排序</h2>
<p>它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。</p>
<p>一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下：</p>
<p>从第一个元素开始，该元素可以认为已经被排序<br>
取出下一个元素，在已经排序的元素序列中从后向前扫描<br>
如果该元素（已排序）大于新元素，将该元素移到下一位置<br>
重复步骤3，直到找到已排序的元素小于或者等于新元素的位置<br>
将新元素插入到该位置后<br>
重复步骤2~5</p>
<pre><code class="language-java">public int[] MySort(int[] arr) {
        int temp;
        //从下标为1的元素开始选择合适的位置插入，因为下标为0的只有一个元素，默认是有序的
        for (int i = 1; i &lt; arr.length; i++) {
            //记录要插入的数据
            temp = arr[i];
            //从已经排序的序列最右边开始比较，找到比其小的数
            int j = i;
            while (j &gt; 0 &amp;&amp; temp &lt; arr[j - 1]) {
                arr[j] = arr[j - 1];
                j--;
            }
            //存在比其小的数，插入
            if (j != i) {
                arr[j] = temp;
            }
        }
        return arr;
}
</code></pre>
<h2 id="希尔排序">希尔排序</h2>
<p>也称递减增量排序算法，是插入排序的一种更高效的改进版本。<br>
希尔排序是基于插入排序的以下两点性质而提出改进方法的：</p>
<ol>
<li>插入排序在对几乎已经排好序的数据操作时，效率高，即可以达到线性排序的效率</li>
<li>但插入排序一般来说是低效的，因为插入排序每次只能将数据移动一位</li>
</ol>
<p>希尔排序通过将比较的全部元素分为几个区域来提升插入排序的性能。这样可以让一个元素可以一次性地朝最终位置前进一大步。然后算法再取越来越小的步长进行排序，算法的最后一步就是普通的插入排序，但是到了这步，需排序的数据几乎是已排好的了（此时插入排序较快）。</p>
<p>假设有一个很小的数据在一个已按升序排好序的数组的末端。如果用复杂度为O(n2)的排序（冒泡排序或插入排序），可能会进行n次的比较和交换才能将该数据移至正确位置。而希尔排序会用较大的步长移动数据，所以小数据只需进行少数比较和交换即可到正确位置。</p>
<p>一个更好理解的希尔排序实现：将数组列在一个表中并对列排序（用插入排序）。重复这过程，不过每次用更长的列来进行。最后整个表就只有一列了。将数组转换至表是为了更好地理解这算法，算法本身仅仅对原数组进行排序（通过增加索引的步长，例如是用i += step_size而不是i++ ）。</p>
<p>例如，假设有这样一组数[ 13 14 94 33 82 25 59 94 65 23 45 27 73 25 39 10 ]，如果我们以步长为5开始进行排序，我们可以通过将这列表放在有5列的表中来更好地描述算法，这样他们就应该看起来是这样：</p>
<blockquote>
<p>13 14 94 33 82<br>
25 59 94 65 23<br>
45 27 73 25 39<br>
10</p>
</blockquote>
<p>然后我们对每列进行排序：</p>
<blockquote>
<p>10 14 73 25 23<br>
13 27 94 33 39<br>
25 59 94 65 82<br>
45</p>
</blockquote>
<p>将上述四行数字，依序接在一起时我们得到：[ 10 14 73 25 23 13 27 94 33 39 25 59 94 65 82 45 ].这时10已经移至正确位置了，然后再以3为步长进行排序：</p>
<blockquote>
<p>10 14 73<br>
25 23 13<br>
27 94 33<br>
39 25 59<br>
94 65 82<br>
45</p>
</blockquote>
<p>排序之后变为：</p>
<blockquote>
<p>10 14 13<br>
25 23 33<br>
27 25 59<br>
39 65 73<br>
45 94 82<br>
94</p>
</blockquote>
<p>最后以1步长进行排序（此时就是简单的插入排序了）。</p>
<h3 id="步长序列">步长序列</h3>
<p>步长的选择是希尔排序的重要部分。只要最终步长为1任何步长序列都可以工作。算法最开始以一定的步长进行排序。然后会继续以一定步长进行排序，最终算法以步长为1进行排序。当步长为1时，算法变为普通插入排序，这就保证了数据一定会被排序。</p>
<p>Donald Shell最初建议步长选择为n/2并且对步长取半直到步长达到1。虽然这样取可以比O(n<sup>2</sup>)类的算法（插入排序）更好，但这样仍然有减少平均时间和最差时间的余地。</p>
<pre><code class="language-java">public int[] MySort(int[] arr) {
        int length = arr.length;
        int temp;
        for (int step = length / 2; step &gt;= 1; step /= 2) {
            for (int i = step; i &lt; length; i++) {
                temp = arr[i];
                int j = i - step;
                while (j &gt;= 0 &amp;&amp; arr[j] &gt; temp) {
                    arr[j + step] = arr[j];
                    j -= step;
                }
                arr[j + step] = temp;
            }
        }
        return arr;
}
</code></pre>
<h2 id="归并排序">归并排序</h2>
<p>是创建在归并操作上的一种有效的排序算法，效率为 O(n log n)<br>
采用分治法（Divide and Conquer）的一个非常典型的应用，且各层分治递归可以同时进行。</p>
<p>采用分治法:</p>
<p>分割：递归地把当前序列平均分割成两半。<br>
集成：在保持元素顺序的同时将上一步得到的子序列集成到一起（归并）。</p>
<h3 id="归并操作">归并操作</h3>
<p>归并操作（merge），也叫归并算法，指的是将两个已经排序的序列合并成一个序列的操作。归并排序算法依赖归并操作。</p>
<p>递归法（Top-down）</p>
<ol>
<li>申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列</li>
<li>设定两个指针，最初位置分别为两个已经排序序列的起始位置</li>
<li>比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置</li>
<li>重复步骤3直到某一指针到达序列尾</li>
<li>将另一序列剩下的所有元素直接复制到合并序列尾</li>
</ol>
<p>迭代法（Bottom-up）<br>
原理如下（假设序列共有n个元素）：</p>
<ol>
<li>将序列每相邻两个数字进行归并操作，形成 ceil(n/2)个序列，排序后每个序列包含两/一个元素</li>
<li>若此时序列数不是1个则将上述序列再次归并，形成ceil(n/4)个序列，每个序列包含四/三个元素</li>
<li>重复步骤2，直到所有元素排序完毕，即序列数为1</li>
</ol>
<p>递归版：</p>
<pre><code class="language-java"></code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[反转链表]]></title>
        <id>https://cc1024201.github.io/post/fan-zhuan-lian-biao/</id>
        <link href="https://cc1024201.github.io/post/fan-zhuan-lian-biao/">
        </link>
        <updated>2021-06-08T13:13:50.000Z</updated>
        <content type="html"><![CDATA[<h1 id="描述">描述</h1>
<p>输入一个链表，反转链表后，输出链表的表头</p>
<h1 id="示例1">示例1</h1>
<blockquote>
<p>输入：{1，2，3}<br>
返回值：{3，2，1}</p>
</blockquote>
<h2 id="方法一构造链表">方法一：构造链表</h2>
<p>可以先用一个按照插入顺序排序的集合将单链表每个节点都存起来，然后再构造链表<br>
此方法简单易懂</p>
<pre><code class="language-java">/*
public class ListNode {
    int val;
    ListNode next = null;

    ListNode(int val) {
        this.val = val;
    }
}*/
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

public class Solution {
    public ListNode ReverseList(ListNode head) {
        if (null == head) {
            return null;
        }
        List&lt;ListNode&gt; list = new ArrayList&lt;&gt;();
        while (head != null) {
            list.add(head);
            head = head.next;
        }
        Collections.reverse(list);
        for (ListNode node : list) {
            node.next = null;
        }
        head = list.get(0);
        list.remove(0);
        ListNode currentNode = head;
        while (list.size() &gt; 0) {
            currentNode.next = list.get(0);
            list.remove(0);
            currentNode = currentNode.next;
        }
        return head;
    }
}
</code></pre>
<h2 id="方法二正规解法">方法二：正规解法</h2>
<p>此题想考察的的是：如何调整链表指针，来达到反转链表的目的。</p>
<ol>
<li>初始化：3个指针
<ul>
<li>pre指向已经反转好的链表的最后一个节点，最开始没有反转，所以为null</li>
<li>cur指向待反转链表的第一个节点，最开始第一个节点待反转，所以为head</li>
<li>nex指向待反转链表的第二个节点，目的是保存链表，因为cur改变指向后，后面的链表会失效，所以需要保存</li>
</ul>
</li>
<li>循环执行以下三个操作
<ul>
<li>nex = cur.next, 保存作用</li>
<li>cur.next = pre，没有反转的链表的第一个节点的下个节点是已反转链表的最后一个节点（在此完成反转）</li>
<li>pre = cur, cur = nex; 指针后移， 操作下一个未反转链表的第一个节点</li>
</ul>
</li>
</ol>
<pre><code class="language-java">/*
public class ListNode {
  int val;
  ListNode next = null;

  ListNode(int val) {
      this.val = val;
  }
}*/
public class Solution {
  public ListNode ReverseList(ListNode head) {
      ListNode pre = null;
      ListNode cur = head;
      ListNode nex = null;
      while (null != cur) {
          nex = cur.next; //记录当前节点的下一个节点位置
          cur.next = pre; //让当前节点指向前一个节点位置，完成反转
          pre = cur; //pre往右走
          cur = nex; //当前节点往右走
      }
      return pre;
  }
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[阿里云 Spring Boot 脚手架]]></title>
        <id>https://cc1024201.github.io/post/a-li-yun-spring-boot-jiao-shou-jia/</id>
        <link href="https://cc1024201.github.io/post/a-li-yun-spring-boot-jiao-shou-jia/">
        </link>
        <updated>2021-05-26T15:23:31.000Z</updated>
        <content type="html"><![CDATA[<p>https://start.aliyun.com/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello World]]></title>
        <id>https://cc1024201.github.io/post/about/</id>
        <link href="https://cc1024201.github.io/post/about/">
        </link>
        <updated>2019-01-25T11:09:48.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>欢迎来到我的小站呀，很高兴遇见你！🤝</p>
</blockquote>
<h2 id="关于本站">🏠 关于本站</h2>
<h2 id="博主是谁">👨‍💻 博主是谁</h2>
<h2 id="兴趣爱好">⛹ 兴趣爱好</h2>
<h2 id="联系我呀">📬 联系我呀</h2>
]]></content>
    </entry>
</feed>